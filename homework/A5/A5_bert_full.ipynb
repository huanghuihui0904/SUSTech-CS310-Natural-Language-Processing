{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 5 (part 2): Pretraining BERT with on a Full Dataset\n",
    "\n",
    "You should re-use the code from A5_bert_toy.ipynb. For clarity, you are suggested to put the code for model definition in a separate file, e.g., model.py, and import it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import random\n",
    "from typing import List, Dict\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import Embedding,ScaledDotProductAttention,MultiHeadAttention,PoswiseFeedForwardNet,EncoderLayer,BERT\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_vocab(file_paths):\n",
    "    sentences=[]\n",
    "    for file_path in file_paths:\n",
    "    # Initialize a set to store unique words\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            sentences.extend(text.replace('\\n',\"\"))\n",
    "\n",
    "    word_types = set(list(sentences))\n",
    "\n",
    "    word_to_id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "    for i, w in enumerate(word_types):\n",
    "        word_to_id[w] = i + 4\n",
    "    id_to_word = {i: w for i, w in enumerate(word_to_id)}\n",
    "    vocab_size = len(word_to_id)\n",
    "\n",
    "\n",
    "    return word_to_id, id_to_word, vocab_size\n",
    "\n",
    "def tokenize_sentences(file_path, word_to_id):\n",
    "\n",
    "    tokens_list = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        sentences=text.split('\\n')\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            tokens=[]\n",
    "            for s in list(sentence):\n",
    "                tokens.append(word_to_id[s])\n",
    "            tokens_list.append(tokens)\n",
    "\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 1529\n",
      "word_to_id {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3, '呺': 4, '战': 5, '设': 6, '滑': 7, '閒': 8, '时': 9, '避': 10, '恤': 11, '謷': 12, '矣': 13, '鼷': 14, '复': 15, '徙': 16, '仆': 17, '失': 18, '停': 19, '疲': 20, '倍': 21, '利': 22, '知': 23, '对': 24, '暴': 25, '挑': 26, '刑': 27, '晦': 28, '术': 29, '连': 30, '可': 31, '饥': 32, '矰': 33, '仞': 34, '偏': 35, '排': 36, '撮': 37, '彰': 38, '诸': 39, '振': 40, '池': 41, '始': 42, '鹪': 43, '胡': 44, '斗': 45, '维': 46, '栎': 47, '观': 48, '商': 49, '把': 50, '同': 51, '翣': 52, '？': 53, '猎': 54, '危': 55, '绝': 56, '理': 57, '旷': 58, '华': 59, '雾': 60, '毁': 61, '鸡': 62, '未': 63, '咬': 64, '巫': 65, '勤': 66, '象': 67, '儵': 68, '地': 69, '譹': 70, '坠': 71, '榆': 72, '慎': 73, '椿': 74, '军': 75, '杀': 76, '成': 77, '域': 78, '礴': 79, '争': 80, '黜': 81, '顾': 82, '涕': 83, '饮': 84, '泣': 85, '郑': 86, '誉': 87, '喁': 88, '斄': 89, '洫': 90, '汝': 91, '猵': 92, '耆': 93, '黥': 94, '繲': 95, '管': 96, '任': 97, '多': 98, '后': 99, '履': 100, '惑': 101, '暖': 102, '莛': 103, '西': 104, '豚': 105, '靡': 106, '操': 107, '姚': 108, '示': 109, '幻': 110, '藏': 111, '絜': 112, '蓏': 113, '百': 114, '骑': 115, '螳': 116, '頯': 117, '衡': 118, '轧': 119, '运': 120, '耦': 121, '浮': 122, '秋': 123, '鄙': 124, '苦': 125, '能': 126, '琴': 127, '外': 128, '平': 129, '晋': 130, '令': 131, '递': 132, '菌': 133, '许': 134, '矼': 135, '仅': 136, '天': 137, '弋': 138, '智': 139, '勿': 140, '此': 141, '瘿': 142, '鹩': 143, '恃': 144, '乃': 145, '鸦': 146, '仑': 147, '今': 148, '捷': 149, '问': 150, '叔': 151, '庶': 152, '嗟': 153, '鼻': 154, '材': 155, '遇': 156, '负': 157, '似': 158, '骸': 159, '席': 160, '趋': 161, '雌': 162, '哇': 163, '蓼': 164, '求': 165, '禹': 166, '胶': 167, '贻': 168, '鱼': 169, '赍': 170, '沾': 171, '宎': 172, '翛': 173, '尧': 174, '舜': 175, '瞻': 176, '脣': 177, '正': 178, '何': 179, '贡': 180, '粮': 181, '疏': 182, '牛': 183, '改': 184, '疑': 185, '径': 186, '黄': 187, '辞': 188, '田': 189, '戏': 190, '附': 191, '野': 192, '见': 193, '偕': 194, '志': 195, '炎': 196, '卓': 197, '爪': 198, '蜩': 199, '凤': 200, '偻': 201, '昧': 202, '俯': 203, '竹': 204, '奚': 205, '巢': 206, '封': 207, '觉': 208, '游': 209, '琢': 210, '画': 211, '丛': 212, '役': 213, '诏': 214, '留': 215, '蕉': 216, '而': 217, '浸': 218, '以': 219, '洴': 220, '病': 221, '跃': 222, '忒': 223, '况': 224, '老': 225, '给': 226, '斤': 227, '齐': 228, '恂': 229, '坏': 230, '彀': 231, '由': 232, '指': 233, '瞒': 234, '策': 235, '规': 236, '廉': 237, '埃': 238, '使': 239, '洗': 240, '就': 241, '批': 242, '二': 243, '陆': 244, '遥': 245, '刀': 246, '子': 247, '厚': 248, '尔': 249, '盟': 250, '骇': 251, '照': 252, '故': 253, '鳅': 254, '禄': 255, '申': 256, '楚': 257, '梁': 258, '鸟': 259, '楹': 260, '妙': 261, '爨': 262, '夜': 263, '充': 264, '鲵': 265, '六': 266, '先': 267, '箕': 268, '然': 269, '抟': 270, '终': 271, '水': 272, '冰': 273, '飘': 274, '斩': 275, '蛆': 276, '宅': 277, '巨': 278, '才': 279, '柏': 280, '毫': 281, '於': 282, '道': 283, '方': 284, '悍': 285, '索': 286, '颡': 287, '音': 288, '。': 289, '两': 290, '棘': 291, '浆': 292, '陋': 293, '祥': 294, '落': 295, '窖': 296, '特': 297, '社': 298, '者': 299, '宇': 300, '宜': 301, '车': 302, '海': 303, '言': 304, '苶': 305, '凄': 306, '日': 307, '尘': 308, '孺': 309, '芥': 310, '独': 311, '聪': 312, '端': 313, '脗': 314, '医': 315, '尽': 316, '椁': 317, '卫': 318, '凿': 319, '长': 320, '栝': 321, '洼': 322, '笑': 323, '叹': 324, '栗': 325, '临': 326, '候': 327, '妾': 328, '挈': 329, '简': 330, '女': 331, '蝍': 332, '秦': 333, '摇': 334, '叶': 335, '达': 336, '婴': 337, '蒿': 338, '缦': 339, '眇': 340, '贫': 341, '震': 342, '射': 343, '阖': 344, '悦': 345, '安': 346, '胜': 347, '动': 348, '夭': 349, '熏': 350, '荤': 351, '图': 352, '溺': 353, '议': 354, '：': 355, '壹': 356, '鬼': 357, '守': 358, '戚': 359, '万': 360, '机': 361, '暑': 362, '修': 363, '夕': 364, '涸': 365, '识': 366, '向': 367, '裂': 368, '俄': 369, '符': 370, '色': 371, '诊': 372, '恐': 373, '莫': 374, '予': 375, '随': 376, '虻': 377, '碎': 378, '吟': 379, '斥': 380, '济': 381, '受': 382, '露': 383, '良': 384, '固': 385, '密': 386, '灾': 387, '蚹': 388, '隹': 389, '通': 390, '羽': 391, '井': 392, '俗': 393, '乡': 394, '鲲': 395, '作': 396, '邪': 397, '狗': 398, '闻': 399, '尼': 400, '石': 401, '半': 402, '怪': 403, '波': 404, '丸': 405, '断': 406, '蛇': 407, '岂': 408, '依': 409, '困': 410, '芘': 411, '吹': 412, '乐': 413, '拊': 414, '诚': 415, '好': 416, '量': 417, '论': 418, '于': 419, '是': 420, '耳': 421, '愚': 422, '县': 423, '自': 424, '帛': 425, '朝': 426, '态': 427, '为': 428, '荆': 429, '轴': 430, '谨': 431, '鹏': 432, '迁': 433, '顶': 434, '偊': 435, '隙': 436, '佚': 437, '遂': 438, '其': 439, '支': 440, '刻': 441, '迷': 442, '渐': 443, '人': 444, '彼': 445, '它': 446, '疣': 447, '往': 448, '枢': 449, '谪': 450, '躬': 451, '、': 452, '广': 453, '奇': 454, '距': 455, '亏': 456, '忿': 457, '吸': 458, '应': 459, '舞': 460, '又': 461, '寂': 462, '若': 463, '胥': 464, '庭': 465, '爱': 466, '破': 467, '迎': 468, '全': 469, '阴': 470, '服': 471, '疠': 472, '报': 473, '火': 474, '昔': 475, '餐': 476, '凝': 477, '和': 478, '刍': 479, '宁': 480, '保': 481, '历': 482, '徨': 483, '睢': 484, '柚': 485, '虞': 486, '耀': 487, '焚': 488, '敬': 489, '躇': 490, '愦': 491, '虫': 492, '桎': 493, '父': 494, '卜': 495, '矩': 496, '掊': 497, '桂': 498, '委': 499, '犁': 500, '体': 501, '儿': 502, '鸮': 503, '籁': 504, '，': 505, '聃': 506, '谅': 507, '舐': 508, '胆': 509, '回': 510, '丘': 511, '去': 512, '攘': 513, '朴': 514, '缺': 515, '赅': 516, '辍': 517, '释': 518, '激': 519, '雪': 520, '杯': 521, '兽': 522, '羊': 523, '绰': 524, '童': 525, '腐': 526, '锺': 527, '怫': 528, '寝': 529, '逍': 530, '妇': 531, '及': 532, '尸': 533, '毕': 534, '得': 535, '坳': 536, '三': 537, '谷': 538, '穴': 539, '母': 540, '政': 541, '起': 542, '免': 543, '养': 544, '患': 545, '伪': 546, '东': 547, '上': 548, '吉': 549, '罪': 550, '没': 551, '语': 552, '襟': 553, '儒': 554, '介': 555, '甚': 556, '竞': 557, '寄': 558, '眹': 559, '将': 560, '征': 561, '沉': 562, '并': 563, '光': 564, '春': 565, '臂': 566, '余': 567, '徒': 568, '坐': 569, '且': 570, '末': 571, '啮': 572, '信': 573, '顼': 574, '铸': 575, '楸': 576, '忠': 577, '臼': 578, '朋': 579, '刁': 580, '（': 581, '槁': 582, '裹': 583, '杂': 584, '浪': 585, '授': 586, '八': 587, '祸': 588, '樊': 589, '禅': 590, '傍': 591, '荧': 592, '货': 593, '愈': 594, '测': 595, '眴': 596, '取': 597, '鸠': 598, '施': 599, '救': 600, '福': 601, '亢': 602, '呴': 603, '沫': 604, '厌': 605, '缄': 606, '昏': 607, '夷': 608, '业': 609, '荐': 610, '丑': 611, '容': 612, '林': 613, '要': 614, '踊': 615, '择': 616, '灰': 617, '九': 618, '皆': 619, '炙': 620, '真': 621, '斧': 622, '倚': 623, '效': 624, '有': 625, '环': 626, '伏': 627, '豫': 628, '据': 629, '飞': 630, '丽': 631, '周': 632, '审': 633, '右': 634, '逆': 635, '藩': 636, '炉': 637, '昆': 638, '黻': 639, '命': 640, '相': 641, '结': 642, '弱': 643, '督': 644, '众': 645, '铘': 646, '趾': 647, '弃': 648, '毛': 649, '诡': 650, '洛': 651, '挠': 652, '匠': 653, '少': 654, '酒': 655, '河': 656, '约': 657, '调': 658, '拘': 659, '出': 660, '进': 661, '条': 662, '伤': 663, '伐': 664, '开': 665, '昭': 666, '美': 667, '废': 668, '阏': 669, '衣': 670, '忮': 671, '芧': 672, '扶': 673, '丁': 674, '也': 675, '镜': 676, '驾': 677, '卒': 678, '鹿': 679, '公': 680, '汾': 681, '曼': 682, '害': 683, '置': 684, '悗': 685, '酲': 686, '孙': 687, '损': 688, '殃': 689, '辩': 690, '不': 691, '宗': 692, '甫': 693, '所': 694, '轮': 695, '带': 696, '勇': 697, '喉': 698, '岁': 699, '清': 700, '状': 701, '手': 702, '《': 703, '古': 704, '聚': 705, '猗': 706, '深': 707, '肌': 708, '爝': 709, '吾': 710, '式': 711, '泽': 712, '因': 713, '盲': 714, '盖': 715, '热': 716, '赘': 717, '载': 718, '官': 719, '蜃': 720, '试': 721, '嘻': 722, '辙': 723, '武': 724, '首': 725, '占': 726, '富': 727, '怵': 728, '副': 729, '诗': 730, '旬': 731, '祭': 732, '偶': 733, '列': 734, '栩': 735, '每': 736, '狄': 737, '栋': 738, '承': 739, '死': 740, '祝': 741, '梨': 742, '狌': 743, '韦': 744, '扰': 745, '蒸': 746, '感': 747, '苟': 748, '青': 749, '泉': 750, '谓': 751, '捶': 752, '螂': 753, '告': 754, '腹': 755, '踵': 756, '镆': 757, '翱': 758, '在': 759, '背': 760, '寇': 761, '枝': 762, '糠': 763, '市': 764, '君': 765, '诵': 766, '庖': 767, '闉': 768, '鸱': 769, '屈': 770, '卷': 771, '北': 772, '壶': 773, '寓': 774, '纽': 775, '伛': 776, '梏': 777, '堕': 778, '制': 779, '穿': 780, '瓠': 781, '阳': 782, '名': 783, '寿': 784, '蠹': 785, '抢': 786, '彻': 787, '响': 788, '葵': 789, '闭': 790, '遭': 791, '崩': 792, '肝': 793, '嗅': 794, '味': 795, '煎': 796, '藉': 797, '务': 798, '趣': 799, '雄': 800, '白': 801, '浑': 802, '面': 803, '盍': 804, '滀': 805, '代': 806, '如': 807, '鬻': 808, '腾': 809, '己': 810, '必': 811, '五': 812, '疾': 813, '穷': 814, '綦': 815, '稽': 816, '窃': 817, '暮': 818, '蝶': 819, '涯': 820, '舟': 821, '化': 822, '葆': 823, '亡': 824, '迹': 825, '邴': 826, '闵': 827, '蚊': 828, '綮': 829, '悲': 830, '满': 831, '编': 832, '隐': 833, '沌': 834, '欲': 835, '疵': 836, '忧': 837, '冯': 838, '狙': 839, '喜': 840, '喘': 841, '舆': 842, '纶': 843, '砉': 844, '凡': 845, '采': 846, '足': 847, '弹': 848, '关': 849, '斯': 850, '荚': 851, '内': 852, '薪': 853, '宫': 854, '一': 855, '刃': 856, '毒': 857, '师': 858, '虑': 859, '畜': 860, '歌': 861, '共': 862, '橘': 863, '瞿': 864, '山': 865, '叫': 866, '诅': 867, '江': 868, '翅': 869, '匡': 870, '訾': 871, '愿': 872, '柱': 873, '友': 874, '艾': 875, '群': 876, '度': 877, '唯': 878, '蕲': 879, '粃': 880, '魏': 881, '早': 882, '伟': 883, '季': 884, '贯': 885, '瞽': 886, '涂': 887, '桀': 888, '室': 889, '浅': 890, '姑': 891, '直': 892, '听': 893, '沮': 894, '几': 895, '重': 896, '溢': 897, '舍': 898, '主': 899, '与': 900, '舂': 901, '归': 902, '皞': 903, '驰': 904, '梧': 905, '術': 906, '小': 907, '乘': 908, '引': 909, '用': 910, '神': 911, '巧': 912, '楂': 913, '纪': 914, '央': 915, '近': 916, '行': 917, '越': 918, '妖': 919, '致': 920, '讴': 921, '恒': 922, '谁': 923, '腰': 924, '辟': 925, '恢': 926, '节': 927, '民': 928, '息': 929, '豢': 930, '隶': 931, '虽': 932, '合': 933, '声': 934, '工': 935, '澼': 936, '葬': 937, '诟': 938, '咸': 939, '屡': 940, '迟': 941, '新': 942, '鸣': 943, '杙': 944, '块': 945, '脰': 946, '亲': 947, '魂': 948, '狂': 949, '灵': 950, '茹': 951, '望': 952, '贤': 953, '踌': 954, '门': 955, '步': 956, '矢': 957, '丧': 958, '宋': 959, '玉': 960, '聂': 961, '积': 962, '俱': 963, '喻': 964, '奏': 965, '张': 966, '臧': 967, '吊': 968, '肤': 969, '膏': 970, '解': 971, '侍': 972, '汉': 973, '祀': 974, '弟': 975, '居': 976, '助': 977, '常': 978, '高': 979, '强': 980, '位': 981, '鴳': 982, '帝': 983, '刖': 984, '系': 985, '贷': 986, '焉': 987, '针': 988, '远': 989, '朔': 990, '冶': 991, '氾': 992, '适': 993, '慹': 994, '善': 995, '彷': 996, '羿': 997, '会': 998, '攻': 999, '曩': 1000, '至': 1001, '殆': 1002, '月': 1003, '唱': 1004, '十': 1005, '骀': 1006, '计': 1007, '忽': 1008, '騞': 1009, '孔': 1010, '筐': 1011, '仲': 1012, '辱': 1013, '生': 1014, '汤': 1015, '竟': 1016, '颜': 1017, '明': 1018, '贵': 1019, '纯': 1020, '冬': 1021, '流': 1022, '堂': 1023, '辕': 1024, '治': 1025, '马': 1026, '传': 1027, '闲': 1028, '吴': 1029, '苍': 1030, '族': 1031, '非': 1032, '风': 1033, '怒': 1034, '王': 1035, '来': 1036, '间': 1037, '府': 1038, '蛄': 1039, '谍': 1040, '触': 1041, '孝': 1042, '旁': 1043, '选': 1044, '谟': 1045, '陶': 1046, '畛': 1047, '奄': 1048, '加': 1049, '惴': 1050, '蕴': 1051, '境': 1052, '旱': 1053, '尻': 1054, '衍': 1055, '忘': 1056, '口': 1057, '乱': 1058, '定': 1059, '哭': 1060, '中': 1061, '圈': 1062, '痔': 1063, '漆': 1064, '嘘': 1065, '臣': 1066, '樽': 1067, '牧': 1068, '物': 1069, '犯': 1070, '饱': 1071, '豹': 1072, '狸': 1073, '盛': 1074, '异': 1075, '视': 1076, '孟': 1077, '谲': 1078, '技': 1079, '献': 1080, '蓬': 1081, '嗛': 1082, '遁': 1083, '卧': 1084, '鼓': 1085, '形': 1086, '蔽': 1087, '国': 1088, '鉴': 1089, '俎': 1090, '庸': 1091, '颛': 1092, '跳': 1093, '衰': 1094, '本': 1095, '属': 1096, '夫': 1097, '围': 1098, '朕': 1099, '户': 1100, '或': 1101, '藾': 1102, '悔': 1103, '变': 1104, '入': 1105, '畏': 1106, '导': 1107, '灌': 1108, '确': 1109, '颐': 1110, '萌': 1111, '枋': 1112, '罟': 1113, '让': 1114, '軱': 1115, '类': 1116, '实': 1117, '畸': 1118, '丈': 1119, '崔': 1120, '士': 1121, '挤': 1122, '衔': 1123, '脍': 1124, '反': 1125, '肩': 1126, '宙': 1127, '假': 1128, '泄': 1129, '撄': 1130, '烂': 1131, '龟': 1132, '怀': 1133, '冱': 1134, '暇': 1135, '纷': 1136, '涽': 1137, '急': 1138, '剥': 1139, '既': 1140, '霖': 1141, '微': 1142, '兀': 1143, '转': 1144, '幸': 1145, '构': 1146, '垂': 1147, '称': 1148, '之': 1149, '旦': 1150, '欢': 1151, '窅': 1152, '敢': 1153, '法': 1154, '南': 1155, '驷': 1156, '鲜': 1157, '粟': 1158, '液': 1159, '私': 1160, '彭': 1161, '瘳': 1162, '学': 1163, '章': 1164, '缘': 1165, '召': 1166, '云': 1167, '焦': 1168, '存': 1169, '袭': 1170, '纣': 1171, '棺': 1172, '发': 1173, '》': 1174, '饭': 1175, '斵': 1176, '剖': 1177, '壮': 1178, '傅': 1179, '前': 1180, '宾': 1181, '窾': 1182, '敖': 1183, '讵': 1184, '戮': 1185, '尊': 1186, '餬': 1187, '灭': 1188, '当': 1189, '冲': 1190, '徵': 1191, '詹': 1192, '眉': 1193, '凶': 1194, '斋': 1195, '渴': 1196, '亦': 1197, '核': 1198, '扈': 1199, '无': 1200, '寐': 1201, '钧': 1202, '貌': 1203, '顺': 1204, '违': 1205, '说': 1206, '赋': 1207, '藐': 1208, '荣': 1209, '立': 1210, '益': 1211, '买': 1212, '案': 1213, '过': 1214, '举': 1215, '漠': 1216, '培': 1217, '芒': 1218, '污': 1219, '伯': 1220, '阕': 1221, '器': 1222, '崖': 1223, '金': 1224, '消': 1225, '止': 1226, '泰': 1227, '蘧': 1228, '控': 1229, '！': 1230, '冥': 1231, '种': 1232, '嫱': 1233, '荡': 1234, '太': 1235, '妻': 1236, '尝': 1237, '噫': 1238, '钟': 1239, '雨': 1240, '哉': 1241, '略': 1242, '难': 1243, '礼': 1244, '比': 1245, '罔': 1246, '氏': 1247, '杜': 1248, '仁': 1249, '兵': 1250, '徐': 1251, '脊': 1252, '湖': 1253, '束': 1254, '曰': 1255, '久': 1256, '捐': 1257, '左': 1258, '啄': 1259, '惊': 1260, '仰': 1261, '殷': 1262, '叱': 1263, '甕': 1264, '权': 1265, '句': 1266, '惠': 1267, '醉': 1268, '桓': 1269, '蟪': 1270, '精': 1271, '黮': 1272, '谋': 1273, '豕': 1274, '事': 1275, '遗': 1276, '世': 1277, '速': 1278, '启': 1279, '请': 1280, '景': 1281, '播': 1282, '踦': 1283, '翏': 1284, '树': 1285, '坚': 1286, '经': 1287, '接': 1288, '思': 1289, '角': 1290, '降': 1291, '妄': 1292, '媚': 1293, '绳': 1294, '托': 1295, '怛': 1296, '待': 1297, '侧': 1298, '埌': 1299, '聋': 1300, '提': 1301, '胼': 1302, '拱': 1303, '虎': 1304, '补': 1305, '客': 1306, '登': 1307, '侯': 1308, '川': 1309, '黼': 1310, '庄': 1311, '禺': 1312, '曾': 1313, '食': 1314, '暗': 1315, '哀': 1316, '卵': 1317, '意': 1318, '次': 1319, '芚': 1320, '肯': 1321, '嗌': 1322, '壤': 1323, '年': 1324, '孰': 1325, '家': 1326, '伦': 1327, '功': 1328, '下': 1329, '劓': 1330, '循': 1331, '追': 1332, '祖': 1333, '熟': 1334, '目': 1335, '謋': 1336, '姬': 1337, '窍': 1338, '轻': 1339, '讲': 1340, '离': 1341, '七': 1342, '曲': 1343, '痈': 1344, '弗': 1345, '樗': 1346, '败': 1347, '髀': 1348, '便': 1349, '怖': 1350, '覆': 1351, '欺': 1352, '松': 1353, '身': 1354, '卑': 1355, '造': 1356, '寡': 1357, '分': 1358, '蹶': 1359, '梦': 1360, '酌': 1361, '谐': 1362, '则': 1363, '四': 1364, '玄': 1365, '雕': 1366, '肿': 1367, '土': 1368, '匹': 1369, '莽': 1370, '兑': 1371, '鹊': 1372, '干': 1373, '竭': 1374, '桑': 1375, '司': 1376, '觚': 1377, '心': 1378, '蹴': 1379, '集': 1380, '郤': 1381, '畦': 1382, '狐': 1383, '极': 1384, '戒': 1385, '千': 1386, '町': 1387, '药': 1388, '我': 1389, '挟': 1390, '号': 1391, '雷': 1392, '勘': 1393, '产': 1394, '偃': 1395, '疒': 1396, '㼜': 1397, '瓢': 1398, '虚': 1399, '甘': 1400, '嘉': 1401, '）': 1402, '处': 1403, '圆': 1404, '大': 1405, '果': 1406, '鷇': 1407, '猨': 1408, '剋': 1409, '执': 1410, '散': 1411, '劳': 1412, '教': 1413, '壑': 1414, '决': 1415, '夏': 1416, '颠': 1417, '注': 1418, '星': 1419, '惧': 1420, '他': 1421, '徇': 1422, '易': 1423, '麋': 1424, '郭': 1425, '木': 1426, '宿': 1427, '走': 1428, '淡': 1429, '勉': 1430, '交': 1431, '嫠': 1432, '雉': 1433, '活': 1434, '肢': 1435, '肖': 1436, '粗': 1437, '折': 1438, '资': 1439, '勃': 1440, '参': 1441, '絖': 1442, '奈': 1443, '鲁': 1444, '轵': 1445, '嗒': 1446, '倪': 1447, '细': 1448, '根': 1449, '数': 1450, '拂': 1451, '濡': 1452, '御': 1453, '宰': 1454, '割': 1455, '恶': 1456, '龙': 1457, '臃': 1458, '溃': 1459, '拙': 1460, '猴': 1461, '沴': 1462, '骤': 1463, '德': 1464, '涉': 1465, '胸': 1466, '已': 1467, '侔': 1468, '闷': 1469, '翦': 1470, '硎': 1471, '瞑': 1472, '挫': 1473, '情': 1474, '圣': 1475, '謞': 1476, '；': 1477, '鼠': 1478, '垢': 1479, '文': 1480, '里': 1481, '劝': 1482, '諔': 1483, '跂': 1484, '倦': 1485, '兮': 1486, '厉': 1487, '更': 1488, '力': 1489, '床': 1490, '胁': 1491, '枅': 1492, '逄': 1493, '营': 1494, '需': 1495, '欣': 1496, '翔': 1497, '拳': 1498, '逃': 1499, '膝': 1500, '跽': 1501, '期': 1502, '轩': 1503, '气': 1504, '寒': 1505, '义': 1506, '豨': 1507, '从': 1508, '弊': 1509, '淆': 1510, '休': 1511, '墨': 1512, '扬': 1513, '犹': 1514, '击': 1515, '乎': 1516, '圹': 1517, '尾': 1518, '擎': 1519, '湿': 1520, '翼': 1521, '泠': 1522, '渊': 1523, '恣': 1524, '殇': 1525, '寥': 1526, '蒲': 1527, '孽': 1528}\n",
      "train_data [[772, 1231, 625, 169, 505, 439, 783, 428, 395, 289], [395, 1149, 1405, 505, 691, 23, 439, 895, 1386, 1481, 675, 289], [822, 217, 428, 259, 505, 439, 783, 428, 432, 289]]\n",
      "test_data [[439, 1405, 1095, 1458, 1367, 217, 691, 1061, 1294, 1512, 505, 439, 907, 762, 771, 1343, 217, 691, 1061, 236, 496, 289], [1210, 1149, 887, 505, 653, 299, 691, 82, 289], [148, 247, 1149, 304, 505, 1405, 217, 1200, 910, 505, 645, 694, 51, 512, 675, 289]]\n",
      "max_len: 71\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and preprocess data\n",
    "word_to_id, id_to_word, vocab_size = build_vocab(['train.txt','test.raw.txt'])\n",
    "print(\"vocab_size\",vocab_size)\n",
    "print(\"word_to_id\",word_to_id)\n",
    "train_data = tokenize_sentences('train.txt', word_to_id)\n",
    "print(\"train_data\",train_data[:3])\n",
    "test_data = tokenize_sentences('test.raw.txt', word_to_id)\n",
    "print(\"test_data\",test_data[:3])\n",
    "tokens_list=train_data\n",
    "\n",
    "max_len = max(len(seq) for seq in tokens_list)\n",
    "print(\"max_len:\", max_len)\n",
    "MAX_LEN = max_len\n",
    "VOCAB_SIZE=vocab_size\n",
    "batch_size = 6\n",
    "MAX_PRED = 5 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(tokens_list: List[int], batch_size: int, word_to_id: Dict):\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    \n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        sent_a_index, sent_b_index= random.randrange(len(tokens_list)), random.randrange(len(tokens_list))\n",
    "        tokens_a, tokens_b= tokens_list[sent_a_index], tokens_list[sent_b_index]\n",
    "\n",
    "        input_ids = [word_to_id['[CLS]']] + tokens_a + [word_to_id['[SEP]']] + tokens_b + [word_to_id['[SEP]']]\n",
    "        segment_ids = [1] * (1 + len(tokens_a) + 1) + [2] * (len(tokens_b) + 1)\n",
    "\n",
    "        # The following code is used for the Masked Language Modeling (MLM) task.\n",
    "        n_pred =  min(MAX_PRED, max(1, int(round(len(input_ids) * 0.15)))) # Predict at most 15 % of tokens in one sentence\n",
    "        masked_candidates_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word_to_id['[CLS]'] and token != word_to_id['[SEP]']]\n",
    "        random.shuffle(masked_candidates_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in masked_candidates_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            ### START YOUR CODE ###\n",
    "            # Throw a dice to decide if you want to replace the token with [MASK], random word, or remain the same\n",
    "            dice = random.random()\n",
    "            if dice < 0.8:\n",
    "                input_ids[pos] = word_to_id['[MASK]']\n",
    "            else:\n",
    "                dice = random.random()\n",
    "                if dice < 0.5:\n",
    "                    dice = random.random()\n",
    "                    input_ids[pos] = random.randint(0, VOCAB_SIZE-1)\n",
    "            # Otherwise, keep the same token (10% of the time)\n",
    "            ### END YOUR CODE ###\n",
    "\n",
    "        # Make zero paddings\n",
    "        n_pad = MAX_LEN - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero padding (100% - 15%) of thetokens\n",
    "        if MAX_PRED > n_pred:\n",
    "            n_pad = MAX_PRED - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        # The following code is used for the Next Sentence Prediction (NSP) task.\n",
    "        ### START YOUR CODE ###\n",
    "        # Decide if the is_next label is positive or negative, by comparing sent_a_index and sent_b_index\n",
    "        # Don't forget to increment the positive/negative count\n",
    "        # The following code is used for the Next Sentence Prediction (NSP) task.\n",
    "        is_next = sent_a_index + 1 == sent_b_index\n",
    "        if is_next and positive < batch_size // 2:\n",
    "            positive += 1\n",
    "            batch.append((input_ids, segment_ids,  masked_tokens, masked_pos,is_next))\n",
    "        elif not is_next and negative < batch_size // 2:\n",
    "            negative += 1\n",
    "            batch.append((input_ids, segment_ids,  masked_tokens, masked_pos,is_next))\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled text:\n",
      "['[CLS]', '三', '年', '[MASK]', '后', '，', '未', '尝', '[MASK]', '[MASK]', '牛', '也', '；', '[SEP]', '以', '汝', '为', '虫', '臂', '乎', '？', '[SEP]']\n",
      "\n",
      "input_ids: tensor([   1,  537, 1324,    3,   99,  505,   63, 1237,    3,    3,  183,  675,\n",
      "        1477,    2,  219,   91,  428,  492,  566, 1516,   53,    2,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "segment_ids: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "masked_tokens: tensor([ 469,  193, 1149,    0,    0])\n",
      "masked_pos: tensor([9, 8, 3, 0, 0])\n",
      "is_next: 0\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "batch = make_batch(tokens_list, batch_size, word_to_id)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, is_next = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "sample = 2\n",
    "\n",
    "print('sampled text:')\n",
    "print([id_to_word[w.item()] for w in input_ids[sample] if id_to_word[w.item()] != '[PAD]'])\n",
    "print()\n",
    "print('input_ids:', input_ids[sample])\n",
    "print('segment_ids:', segment_ids[sample])\n",
    "print('masked_tokens:', masked_tokens[sample])\n",
    "print('masked_pos:', masked_pos[sample])\n",
    "print('is_next:', is_next[sample].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_attn_mask(seq_q, seq_k):\n",
    "    batch_size, seq_len = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1) \n",
    "    return pad_attn_mask.expand(batch_size, seq_len, len_k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_outputs: torch.Size([6, 71, 768])\n",
      "attn: torch.Size([6, 12, 71, 71])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "batch = make_batch(tokens_list, batch_size, word_to_id)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, is_next = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "enc_layer = EncoderLayer()\n",
    "enc_self_attn_mask = get_pad_attn_mask(input_ids, input_ids)\n",
    "embedding = Embedding(VOCAB_SIZE,MAX_LEN)\n",
    "enc_inputs = embedding(input_ids, segment_ids)\n",
    "enc_outputs, attn = enc_layer(enc_inputs, enc_self_attn_mask)\n",
    "\n",
    "print('enc_outputs:', enc_outputs.size())\n",
    "print('attn:', attn.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_lm: torch.Size([6, 5, 1529])\n",
      "logits_clsf: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "batch = make_batch(tokens_list, batch_size, word_to_id)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, is_next = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "model = BERT(VOCAB_SIZE,MAX_LEN)\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "\n",
    "print('logits_lm:', logits_lm.size())\n",
    "print('logits_clsf:', logits_clsf.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 cost = 21.685839\n",
      "Epoch: 0020 cost = 12.189532\n",
      "Epoch: 0030 cost = 7.815399\n",
      "Epoch: 0040 cost = 4.595700\n",
      "Epoch: 0050 cost = 2.177538\n",
      "Epoch: 0060 cost = 1.900656\n",
      "Epoch: 0070 cost = 1.496390\n",
      "Epoch: 0080 cost = 2.591231\n",
      "Epoch: 0090 cost = 1.242195\n",
      "Epoch: 0100 cost = 1.395167\n",
      "Epoch: 0110 cost = 1.195192\n",
      "Epoch: 0120 cost = 1.121188\n",
      "Epoch: 0130 cost = 1.157933\n",
      "Epoch: 0140 cost = 1.079421\n",
      "Epoch: 0150 cost = 1.139462\n",
      "Epoch: 0160 cost = 1.341852\n",
      "Epoch: 0170 cost = 1.196683\n",
      "Epoch: 0180 cost = 1.121397\n",
      "Epoch: 0190 cost = 1.064820\n",
      "Epoch: 0200 cost = 1.076007\n",
      "Epoch: 0210 cost = 1.186879\n",
      "Epoch: 0220 cost = 0.971573\n",
      "Epoch: 0230 cost = 1.113909\n",
      "Epoch: 0240 cost = 0.989813\n",
      "Epoch: 0250 cost = 1.022099\n",
      "Epoch: 0260 cost = 0.941513\n",
      "Epoch: 0270 cost = 1.040421\n",
      "Epoch: 0280 cost = 0.977089\n",
      "Epoch: 0290 cost = 0.885053\n",
      "Epoch: 0300 cost = 1.035623\n",
      "Epoch: 0310 cost = 1.140220\n",
      "Epoch: 0320 cost = 1.000193\n",
      "Epoch: 0330 cost = 0.927826\n",
      "Epoch: 0340 cost = 0.926857\n",
      "Epoch: 0350 cost = 1.012740\n",
      "Epoch: 0360 cost = 0.898276\n",
      "Epoch: 0370 cost = 1.037533\n",
      "Epoch: 0380 cost = 1.029592\n",
      "Epoch: 0390 cost = 1.052258\n",
      "Epoch: 0400 cost = 0.926963\n",
      "Epoch: 0410 cost = 0.975211\n",
      "Epoch: 0420 cost = 1.111709\n",
      "Epoch: 0430 cost = 1.039498\n",
      "Epoch: 0440 cost = 1.061242\n",
      "Epoch: 0450 cost = 0.997689\n",
      "Epoch: 0460 cost = 1.025376\n",
      "Epoch: 0470 cost = 0.989185\n",
      "Epoch: 0480 cost = 0.997969\n",
      "Epoch: 0490 cost = 0.966157\n",
      "Epoch: 0500 cost = 1.046547\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = BERT(VOCAB_SIZE,MAX_LEN)\n",
    "criterion = nn.CrossEntropyLoss() # You can also try two separate losses for each task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch = make_batch(tokens_list, batch_size, word_to_id)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, is_next = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)  # We need to transpose logits_lm to match the shape expected by CrossEntropyLoss\n",
    "    loss_clsf = criterion(logits_clsf, is_next)\n",
    "    loss = loss_lm + loss_clsf\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model.pth\"\n",
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), model_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model =BERT(VOCAB_SIZE,MAX_LEN) \n",
    "# state_dict = torch.load(model_path)\n",
    "# model.load_state_dict(state_dict)  \n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    parts = line.strip().split('\\t')\n",
    "    \n",
    "    if len(parts) == 3:\n",
    "        text, is_next, ground_truth = parts  \n",
    "        \n",
    "        segments = text.split('[SEP]')\n",
    "        \n",
    "        if len(segments) == 3:\n",
    "            sentence_a = segments[0].replace('[CLS]', '').strip()\n",
    "            sentence_b = segments[1].strip()\n",
    "            is_next = int(is_next)\n",
    "            ground_truth_tokens = ground_truth.strip()\n",
    "            \n",
    "            return sentence_a, sentence_b, is_next, ground_truth_tokens\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence, word_to_id):\n",
    "    tokens = [word_to_id.get(word, 0) for word in sentence.split()]\n",
    "    return tokens\n",
    "\n",
    "def read_and_parse_data(file_path, word_to_id):\n",
    "    parsed_data = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines() \n",
    "    \n",
    "    for line in lines:\n",
    "        sentence_a, sentence_b, is_next, ground_truth_words = parse_line(line)\n",
    "        \n",
    "        tokens_a = tokenize_sentence(sentence_a, word_to_id)\n",
    "        tokens_b = tokenize_sentence(sentence_b, word_to_id)\n",
    "        ground_truth_tokens= tokenize_sentence(ground_truth_words, word_to_id)\n",
    "        \n",
    "        parsed_data.append({\n",
    "            'tokens_a': tokens_a,\n",
    "            'tokens_b': tokens_b,\n",
    "            'is_next': is_next,\n",
    "            'ground_truth_tokens': ground_truth_tokens,\n",
    "        })\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, parsed_data, word_to_id):\n",
    "    results = []\n",
    "    nsp_correct_count = 0 \n",
    "    mlm_correct_count=0\n",
    "    total_mask_words=0\n",
    "    total_samples = len(parsed_data)  \n",
    "    \n",
    "    for data in parsed_data:\n",
    "        tokens_a = data['tokens_a']\n",
    "        tokens_b = data['tokens_b']\n",
    "        ground_truth_tokens = data['ground_truth_tokens']\n",
    "        is_next = data['is_next']\n",
    "\n",
    "        \n",
    "        input_ids = [word_to_id['[CLS]']] + tokens_a + [word_to_id['[SEP]']] + tokens_b + [word_to_id['[SEP]']]\n",
    "        segment_ids = [1] * (len(tokens_a) + 2) + [2] * (len(tokens_b) + 1)  # Segment IDs\n",
    "        masked_pos = [i for i, token in enumerate(input_ids) if token == word_to_id.get('[MASK]', 0)]\n",
    "        \n",
    "        input_ids_tensor = torch.LongTensor([input_ids])\n",
    "        segment_ids_tensor = torch.LongTensor([segment_ids])\n",
    "        masked_pos_tensor = torch.LongTensor([masked_pos])\n",
    "\n",
    "        logits_lm, logits_clsf = model(input_ids_tensor, segment_ids_tensor, masked_pos_tensor)\n",
    "\n",
    "        # MLM predictions\n",
    "        predicted_ids = logits_lm.argmax(dim=2).squeeze().data.numpy()\n",
    "        \n",
    "        # print('masked tokens ground truth: ',[id_to_word[pos] for pos in ground_truth_tokens])\n",
    "        # print('predicted masked tokens: ',[id_to_word[pos] for pos in predicted_ids])\n",
    "\n",
    "        # Calculate MLM accuracy\n",
    "        mlm_correct_predictions=0\n",
    "        for i in range(len(predicted_ids)):\n",
    "            if predicted_ids[i]==ground_truth_tokens[i]:\n",
    "                mlm_correct_predictions+=1\n",
    "        mlm_correct_count+=mlm_correct_predictions\n",
    "        total_mask_words+=len(predicted_ids)\n",
    "        mlm_accuracy=mlm_correct_predictions/len(predicted_ids)\n",
    "\n",
    "        # NSP prediction\n",
    "        predicted_is_next = logits_clsf.argmax(dim=1).data.numpy()[0] \n",
    "        nsp_correct = (predicted_is_next == is_next)\n",
    "        if nsp_correct:\n",
    "            nsp_correct_count += 1\n",
    "        results.append({\n",
    "            \"mlm_accuracy\": mlm_accuracy,\n",
    "            \"nsp_correct\": nsp_correct,\n",
    "        })\n",
    "      # Calculate overall MLM and NSP results\n",
    "    overall_mlm_accuracy = mlm_correct_count / total_mask_words  \n",
    "    overall_nsp_accuracy = nsp_correct_count / total_samples  \n",
    "\n",
    "        \n",
    "    \n",
    "    return results,overall_mlm_accuracy,overall_nsp_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tokens_a': [439, 3, 1095, 1458, 1367, 217, 691, 1061, 1294, 1512, 505, 439, 3, 3, 771, 1343, 217, 691, 1061, 236, 496, 289], 'tokens_b': [1210, 1149, 887, 505, 3, 299, 691, 82, 289], 'is_next': 1, 'ground_truth_tokens': [1405, 907, 762, 653]}, {'tokens_a': [148, 247, 1149, 304, 505, 1405, 3, 1200, 910, 505, 645, 694, 51, 512, 675, 3], 'tokens_b': [1311, 247, 1255, 355, 3, 311, 691, 193, 1073, 743, 1516, 53], 'is_next': 1, 'ground_truth_tokens': [217, 289, 247]}, {'tokens_a': [1355, 1354, 217, 627, 505, 219, 327, 3, 299, 1477], 'tokens_b': [547, 104, 1093, 258, 505, 3, 10, 979, 1329, 1477], 'is_next': 1, 'ground_truth_tokens': [1183, 691]}, {'tokens_a': [239, 51, 1516, 1389, 299, 178, 3, 505, 1140, 51, 1516, 1389, 13, 505, 1456, 126, 178, 1149, 3], 'tokens_b': [239, 1075, 1516, 1389, 3, 463, 299, 178, 1149, 3, 1140, 1075, 1516, 1389, 900, 463, 3, 505, 1456, 126, 178, 1149, 53], 'is_next': 1, 'ground_truth_tokens': [1149, 53, 900, 505, 13]}, {'tokens_a': [239, 51, 1516, 3, 3, 463, 299, 178, 1149, 505, 1140, 51, 1516, 1389, 900, 463, 13, 505, 1456, 3, 178, 1149, 53], 'tokens_b': [269, 1363, 1389, 900, 3, 900, 444, 963, 3, 126, 641, 23, 675, 505, 217, 1297, 445, 3, 397, 53], 'is_next': 1, 'ground_truth_tokens': [1389, 900, 126, 463, 691, 675]}, {'tokens_a': [822, 934, 1149, 641, 3, 505, 463, 439, 691, 641, 1297, 289], 'tokens_b': [478, 1149, 3, 3, 1447, 505, 713, 1149, 219, 682, 1055, 505, 694, 219, 814, 1324, 675, 289], 'is_next': 1, 'ground_truth_tokens': [1297, 219, 137]}, {'tokens_a': [179, 751, 478, 1149, 219, 3, 1447, 53], 'tokens_b': [1255, 355, 420, 691, 420, 505, 269, 3, 269, 289], 'is_next': 1, 'ground_truth_tokens': [137, 691]}, {'tokens_a': [420, 463, 1406, 3, 675, 505, 1363, 420, 1149, 1075, 1516, 691, 420, 675, 1197, 1200, 690, 3], 'tokens_b': [269, 463, 1406, 269, 675, 505, 1363, 269, 1149, 1075, 1516, 3, 269, 3, 1197, 1200, 690, 289], 'is_next': 1, 'ground_truth_tokens': [420, 1477, 691, 675]}, {'tokens_a': [1056, 1324, 1056, 1506, 505, 40, 419, 1200, 1016, 3, 3, 774, 39, 1200, 1016, 289], 'tokens_b': [1246, 290, 150, 1281, 1255, 355, 1000, 247, 917, 505, 3, 247, 1226, 1477], 'is_next': 1, 'ground_truth_tokens': [505, 253, 148]}, {'tokens_a': [1000, 247, 569, 505, 148, 247, 542, 3], 'tokens_b': [179, 439, 1200, 3, 107, 900, 53], 'is_next': 1, 'ground_truth_tokens': [289, 297]}, {'tokens_a': [420, 1083, 137, 21, 1474, 505, 1056, 439, 3, 382, 505, 704, 299, 751, 1149, 1083, 3, 1149, 27, 289], 'tokens_b': [3, 1036, 505, 1097, 247, 9, 675, 1477], 'is_next': 1, 'ground_truth_tokens': [694, 137, 993]}, {'tokens_a': [1473, 988, 1025, 95, 505, 847, 3, 1187, 1057, 1477], 'tokens_b': [1085, 851, 1282, 1271, 505, 847, 219, 1314, 1005, 444, 3], 'is_next': 1, 'ground_truth_tokens': [219, 289]}, {'tokens_a': [3, 561, 724, 1121, 505, 1363, 440, 1341, 513, 566, 3, 439, 1037, 1477], 'tokens_b': [548, 625, 1405, 213, 505, 1363, 440, 1341, 219, 625, 978, 813, 691, 3, 1328, 3], 'is_next': 1, 'ground_truth_tokens': [548, 419, 382, 1477]}, {'tokens_a': [548, 900, 221, 299, 1158, 3, 1363, 382, 537, 527, 900, 1005, 1254, 853, 289], 'tokens_b': [1097, 440, 1341, 299, 439, 1086, 299, 3, 1514, 847, 3, 544, 439, 1354, 505, 271, 439, 137, 1324, 505, 461, 224, 3, 3, 439, 1464, 299, 1516, 1230], 'is_next': 1, 'ground_truth_tokens': [505, 505, 219, 440, 1341]}, {'tokens_a': [1010, 247, 3, 257, 505, 257, 3, 1288, 842, 209, 439, 955, 1255, 355, 200, 1486, 200, 1486, 3, 179, 807, 1464, 1149, 1094, 675, 289], 'tokens_b': [1036, 1277, 691, 3, 1297, 505, 448, 1277, 691, 31, 1332, 675, 289], 'is_next': 1, 'ground_truth_tokens': [993, 949, 505, 31]}, {'tokens_a': [137, 1329, 625, 3, 505, 1475, 444, 77, 987, 1477], 'tokens_b': [137, 1329, 3, 283, 505, 1475, 444, 1014, 987, 289], 'is_next': 1, 'ground_truth_tokens': [283, 1200]}, {'tokens_a': [284, 148, 1149, 9, 505, 136, 543, 27, 3, 1230], 'tokens_b': [601, 1339, 1516, 391, 505, 374, 1149, 3, 718, 1477], 'is_next': 1, 'ground_truth_tokens': [987, 23]}, {'tokens_a': [1200, 3, 1149, 1474, 505, 253, 420, 1032, 691, 535, 419, 1354, 289], 'tokens_b': [340, 3, 907, 1241, 505, 694, 219, 1096, 419, 444, 675, 1477], 'is_next': 1, 'ground_truth_tokens': [444, 1516]}, {'tokens_a': [12, 1516, 1405, 1241, 505, 3, 77, 439, 137, 289], 'tokens_b': [1267, 247, 751, 1311, 247, 1255, 355, 444, 3, 1200, 1474, 1516, 53], 'is_next': 1, 'ground_truth_tokens': [311, 253]}, {'tokens_a': [1311, 247, 1255, 3, 269, 289], 'tokens_b': [1267, 3, 1255, 355, 3, 217, 1200, 1474, 505, 179, 219, 751, 1149, 444, 53], 'is_next': 1, 'ground_truth_tokens': [355, 247, 444]}, {'tokens_a': [1311, 247, 1255, 3, 283, 900, 1149, 1203, 505, 3, 900, 1149, 1086, 505, 1456, 535, 691, 3, 1149, 444, 53], 'tokens_b': [1267, 247, 1255, 355, 1140, 3, 1149, 444, 3, 1456, 535, 1200, 1474, 53], 'is_next': 1, 'ground_truth_tokens': [355, 137, 751, 751, 505]}, {'tokens_a': [1255, 355, 510, 3, 1244, 413, 13, 289], 'tokens_b': [1255, 355, 31, 13, 505, 1514, 63, 675, 3], 'is_next': 1, 'ground_truth_tokens': [1056, 289]}, {'tokens_a': [1421, 307, 505, 15, 193, 505, 1255, 355, 510, 3, 13, 289], 'tokens_b': [1255, 355, 3, 751, 675, 53], 'is_next': 1, 'ground_truth_tokens': [1211, 179]}, {'tokens_a': [1255, 355, 510, 3, 1056, 13, 289], 'tokens_b': [1012, 3, 1379, 269, 1255, 355, 179, 751, 569, 1056, 53], 'is_next': 1, 'ground_truth_tokens': [569, 400]}, {'tokens_a': [1017, 510, 1255, 355, 778, 1435, 501, 505, 81, 312, 3, 505, 3, 1086, 512, 139, 505, 51, 419, 3, 390, 505, 141, 751, 569, 3, 289], 'tokens_b': [1012, 400, 1255, 355, 3, 1363, 1200, 3, 675, 505, 822, 1363, 1200, 978, 675, 289], 'is_next': 1, 'ground_truth_tokens': [1018, 1341, 1405, 1056, 51, 416]}, {'tokens_a': [217, 3, 439, 953, 1516, 1230], 'tokens_b': [511, 675, 1280, 3, 217, 99, 675, 289], 'is_next': 1, 'ground_truth_tokens': [1406, 1508]}, {'tokens_a': [247, 842, 900, 247, 1375, 874, 3], 'tokens_b': [3, 1141, 1240, 1005, 307, 3, 247, 842, 1255, 355, 247, 1375, 1002, 221, 13, 1230], 'is_next': 1, 'ground_truth_tokens': [289, 217, 505]}, {'tokens_a': [1136, 217, 207, 1241, 505, 855, 219, 420, 271, 3], 'tokens_b': [1200, 428, 3, 533, 505, 1200, 428, 1273, 3, 505, 1200, 428, 1275, 97, 505, 3, 428, 23, 899, 289], 'is_next': 1, 'ground_truth_tokens': [289, 783, 1038, 1200]}, {'tokens_a': [501, 3, 1200, 814, 505, 217, 209, 1200, 1099, 289], 'tokens_b': [316, 439, 694, 382, 3, 137, 217, 1200, 193, 535, 505, 1197, 1399, 217, 3, 1230], 'is_next': 1, 'ground_truth_tokens': [316, 1516, 1467]}, {'tokens_a': [148, 247, 3, 1405, 1285, 3, 545, 439, 1200, 3, 505, 179, 691, 1285, 1149, 419, 1200, 179, 625, 1149, 394, 505, 453, 374, 1149, 3, 505, 996, 483, 1516, 1200, 428, 439, 1298, 3, 530, 245, 1516, 529, 1084, 439, 1329, 289], 'tokens_b': [137, 1200, 1160, 1351, 505, 69, 1200, 1160, 3, 505, 137, 69, 408, 1160, 341, 1389, 3, 53], 'is_next': 0, 'ground_truth_tokens': [625, 505, 910, 192, 505, 718, 1241]}, {'tokens_a': [1281, 1255, 355, 3, 625, 1297, 217, 269, 299, 397, 53], 'tokens_b': [1311, 247, 1255, 355, 283, 900, 3, 1203, 505, 137, 900, 1149, 1086, 505, 1200, 219, 416, 1456, 3, 663, 439, 3, 289], 'is_next': 0, 'ground_truth_tokens': [710, 1149, 852, 1354]}, {'tokens_a': [247, 842, 1105, 505, 1255, 355, 247, 1149, 861, 730, 505, 179, 253, 3, 420, 53], 'tokens_b': [1002, 1516, 3, 1002, 1516, 1230], 'is_next': 0, 'ground_truth_tokens': [463, 505]}, {'tokens_a': [710, 917, 1381, 1343, 505, 1200, 663, 3, 847, 289], 'tokens_b': [137, 3, 1230], 'is_next': 0, 'ground_truth_tokens': [710, 1516]}, {'tokens_a': [44, 819, 1149, 1360, 428, 632, 900, 3], 'tokens_b': [498, 31, 1314, 505, 253, 3, 1149, 1477], 'is_next': 0, 'ground_truth_tokens': [53, 664]}, {'tokens_a': [68, 900, 1008, 9, 641, 900, 3, 3, 802, 834, 1149, 69, 505, 802, 834, 1297, 1149, 556, 995, 289], 'tokens_b': [710, 694, 1297, 461, 3, 1297, 217, 269, 299, 397, 53], 'is_next': 0, 'ground_truth_tokens': [156, 419, 625]}, {'tokens_a': [1155, 303, 1149, 3, 428, 68, 772, 3, 1149, 983, 428, 1008, 505, 1061, 915, 1149, 983, 428, 802, 834, 289], 'tokens_b': [1467, 3, 505, 1467, 1516, 1230], 'is_next': 0, 'ground_truth_tokens': [983, 303, 1516]}, {'tokens_a': [475, 299, 1311, 632, 1360, 428, 3, 819, 505, 735, 735, 269, 44, 819, 675, 289], 'tokens_b': [3, 23, 632, 1149, 1360, 428, 44, 819, 900, 53], 'is_next': 0, 'ground_truth_tokens': [44, 691]}, {'tokens_a': [307, 3, 855, 1338, 505, 1342, 307, 217, 802, 834, 740, 289], 'tokens_b': [442, 782, 442, 3, 505, 1200, 663, 710, 917, 289], 'is_next': 0, 'ground_truth_tokens': [319, 782]}, {'tokens_a': [3, 444, 219, 1464, 289], 'tokens_b': [865, 1426, 505, 424, 761, 675, 3], 'is_next': 0, 'ground_truth_tokens': [326, 1477]}, {'tokens_a': [625, 691, 97, 439, 934, 217, 161, 1215, 439, 730, 3, 289], 'tokens_b': [424, 964, 993, 195, 3, 1230], 'is_next': 0, 'ground_truth_tokens': [987, 900]}, {'tokens_a': [369, 269, 208, 505, 1363, 1228, 1228, 269, 632, 3, 289], 'tokens_b': [3, 247, 3, 1516, 247, 1149, 911, 505, 1412, 1516, 247, 1149, 1271, 505, 623, 1285, 217, 379, 505, 629, 582, 3, 217, 1472, 289], 'is_next': 0, 'ground_truth_tokens': [675, 148, 128, 905]}, {'tokens_a': [632, 3, 44, 819, 1363, 811, 625, 1358, 13, 289], 'tokens_b': [1001, 247, 1375, 3, 955, 505, 3, 463, 861, 463, 1060, 505, 1085, 127, 1255, 355, 494, 397, 1230], 'is_next': 0, 'ground_truth_tokens': [900, 1149, 1363]}, {'tokens_a': [1255, 3, 710, 1289, 1097, 239, 1389, 1001, 141, 1384, 299, 217, 3, 535, 675, 289], 'tokens_b': [148, 1097, 89, 183, 505, 3, 1405, 463, 1147, 137, 1149, 1167, 289], 'is_next': 0, 'ground_truth_tokens': [355, 1345, 439]}, {'tokens_a': [68, 3, 1008, 1273, 473, 802, 834, 3, 1464, 505, 1255, 355, 3, 619, 625, 1342, 1338, 219, 1076, 893, 1314, 929, 141, 311, 1200, 625, 505, 1237, 721, 319, 1149, 289], 'tokens_b': [141, 3, 751, 1069, 822, 289], 'is_next': 0, 'ground_truth_tokens': [900, 1149, 444, 1149]}, {'tokens_a': [165, 3, 428, 1149, 299, 217, 691, 535, 675, 1230], 'tokens_b': [346, 3, 217, 1403, 1204, 505, 3, 413, 691, 126, 1105, 675, 505, 704, 299, 3, 420, 983, 1149, 423, 971, 289], 'is_next': 0, 'ground_truth_tokens': [439, 9, 1316, 751]}, {'tokens_a': [1064, 31, 910, 505, 253, 1455, 3, 289], 'tokens_b': [993, 3, 505, 1097, 247, 1204, 675, 289], 'is_next': 0, 'ground_truth_tokens': [1149, 512]}, {'tokens_a': [3, 23, 632, 675, 289], 'tokens_b': [444, 619, 3, 625, 910, 1149, 910, 3, 217, 374, 23, 1200, 910, 1149, 910, 675, 289], 'is_next': 0, 'ground_truth_tokens': [691, 23, 505]}, {'tokens_a': [970, 474, 505, 424, 796, 3, 289], 'tokens_b': [1456, 366, 3, 219, 269, 53], 'is_next': 0, 'ground_truth_tokens': [675, 694]}, {'tokens_a': [710, 1297, 407, 388, 199, 3, 397, 53], 'tokens_b': [494, 540, 408, 835, 710, 341, 3, 53], 'is_next': 0, 'ground_truth_tokens': [1521, 1241]}, {'tokens_a': [710, 3, 751, 1200, 1474, 3, 505, 304, 444, 1149, 691, 219, 416, 1456, 3, 663, 3, 1354, 505, 978, 713, 424, 269, 217, 691, 1211, 1014, 675, 289], 'tokens_b': [3, 397, 1230], 'is_next': 0, 'ground_truth_tokens': [694, 299, 852, 439, 540]}, {'tokens_a': [1267, 3, 1255, 355, 691, 1211, 1014, 505, 179, 3, 625, 439, 1354, 53], 'tokens_b': [691, 349, 227, 622, 505, 1069, 3, 683, 3, 505, 1200, 694, 3, 910, 505, 346, 694, 410, 125, 1241, 1230], 'is_next': 0, 'ground_truth_tokens': [247, 219, 1200, 299, 31]}, {'tokens_a': [444, 3, 1230], 'tokens_b': [1311, 247, 1255, 355, 420, 1032, 710, 694, 751, 1474, 675, 3], 'is_next': 0, 'ground_truth_tokens': [1516, 289]}, {'tokens_a': [211, 69, 217, 3, 289], 'tokens_b': [1001, 444, 1149, 910, 1378, 3, 676, 505, 691, 560, 691, 635, 505, 459, 217, 691, 111, 505, 3, 126, 347, 1069, 3, 691, 663, 289], 'is_next': 0, 'ground_truth_tokens': [161, 463, 253, 217]}, {'tokens_a': [588, 896, 1516, 3, 505, 374, 1149, 23, 10, 289], 'tokens_b': [1456, 366, 694, 219, 691, 269, 3], 'is_next': 0, 'ground_truth_tokens': [69, 53]}, {'tokens_a': [233, 3, 419, 428, 853, 505, 474, 1027, 675, 505, 691, 23, 3, 316, 675, 289], 'tokens_b': [141, 126, 428, 1405, 13, 505, 217, 691, 126, 1410, 3, 289], 'is_next': 0, 'ground_truth_tokens': [814, 439, 1478]}, {'tokens_a': [137, 1044, 3, 1149, 1086, 505, 247, 219, 1286, 801, 943, 289], 'tokens_b': [1061, 419, 3, 925, 505, 740, 419, 1246, 1113, 289], 'is_next': 0, 'ground_truth_tokens': [247, 361]}, {'tokens_a': [269, 217, 1001, 141, 1384, 3, 505, 640, 675, 1097, 1230], 'tokens_b': [583, 1175, 217, 448, 3, 1149, 289], 'is_next': 0, 'ground_truth_tokens': [299, 1314]}]\n"
     ]
    }
   ],
   "source": [
    "test_data = read_and_parse_data(\"test.pairs.txt\", word_to_id)\n",
    "\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MLM Accuracy: 0.57%\n",
      "Overall NSP Accuracy: 54.39%\n",
      "Sample 1:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 2:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 3:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 4:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 5:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 6:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 7:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 8:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 9:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 10:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 11:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 12:\n",
      "MLM Accuracy: 50.00%\n",
      "NSP Correct: Yes\n",
      "Sample 13:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 14:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 15:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 16:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 17:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 18:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 19:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 20:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 21:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 22:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 23:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 24:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 25:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 26:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 27:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 28:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 29:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 30:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 31:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 32:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 33:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 34:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 35:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 36:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 37:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 38:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 39:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 40:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 41:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 42:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 43:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 44:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 45:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 46:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 47:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 48:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 49:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 50:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 51:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 52:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 53:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 54:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 55:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n",
      "Sample 56:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: No\n",
      "Sample 57:\n",
      "MLM Accuracy: 0.00%\n",
      "NSP Correct: Yes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results,overall_mlm_accuracy ,overall_nsp_accuracy= evaluate_model(model, test_data, word_to_id)\n",
    "print(f\"Overall MLM Accuracy: {overall_mlm_accuracy:.2%}\")\n",
    "print(f\"Overall NSP Accuracy: {overall_nsp_accuracy:.2%}\")\n",
    "# Display the results\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(f\"MLM Accuracy: {res['mlm_accuracy']:.2%}\")\n",
    "    print(f\"NSP Correct: {'Yes' if res['nsp_correct'] else 'No'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1. Neural Text Classification\n",
    "## CS310 Natural Language Processing\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "You should roughtly follow the structure of the notebook. Add additional cells if you feel needed. \n",
    "\n",
    "You can (and you should) re-use the code from Lab 2. \n",
    "\n",
    "Make sure your code is readable and well-structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import jieba\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data.dataset import random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '说']\n",
      "['保', '姆', '小', '张', '说', '：', '干', '啥', '子', '嘛', '？']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '你', '看', '你', '往', '星', '空', '看', '月', '朦', '胧', '，', '鸟', '朦', '胧']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '咱', '是', '不', '是', '歇', '一', '下', '这', '双', '，', '疲', '惫', '的', '双', '腿', '？']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '快', '把', '我', '累', '死', '了']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '说', '亲', '爱', '的', '大', '姐', '你', '贵', '姓', '啊', '？']\n",
      "['保', '姆', '小', '张', '说', '：', '我', '免', '贵', '姓', '张', '我', '叫', '张', '凤', '姑']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '凤', '姑']\n",
      "['保', '姆', '小', '张', '说', '：', '天', '天', '买', '你', '的', '油', '条', '还', '没', '有', '问', '过', '师', '傅', '，', '你', '贵', '姓', '啊', '？']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '免', '贵', '，', '我', '姓', '刘', '，', '我', '叫', '刘', '建', '军']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '凤', '姑', '姑']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '的', '姑', '啊', '我', '亲', '爱', '的', '姑']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '怎', '么', '那', '么', '别', '扭', '呢', '？']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '自', '从', '见', '了', '你', '以', '后', '我', '的', '这', '个', '生', '活', '，', '我', '的', '这', '个', '事', '业', '发', '生', '了', '翻', '天', '覆', '地', '的', '变', '化']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '只', '要', '你', '的', '身', '影', '，', '从', '那', '个', '胡', '同', '口', '噌', '一', '出', '现']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '的', '这', '个', '心', '里', '头', '就', '像', '那', '个', '油', '锅', '一', '样', '咕', '噜', '噜', '，', '咕', '噜', '噜', '，', '热', '血', '沸', '腾']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '跟', '你', '说', '吧', '咱', '们', '两', '个', '是', '同', '样', '的', '理', '想', '，', '同', '样', '的', '心']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '同', '是', '天', '涯', '沦', '落', '人', '天', '下', '农', '友', '心', '连', '心', '穷', '不', '帮', '穷', '谁', '照', '应', '？']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '都', '说', '了', '半', '天', '了', '，', '你', '倒', '说', '话', '呀']\n",
      "['保', '姆', '小', '张', '说', '：', '我', '是', '在', '用', '不', '在', '乎', '，', '掩', '藏', '真', '心']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '这', '就', '想', '抓', '起', '你', '的', '手', '让', '你', '这', '就', '跟', '我', '走']\n",
      "['保', '姆', '小', '张', '说', '：', '莫', '慌', '喽', '谁', '知', '道', '你', '明', '天', '是', '否', '依', '然', '爱', '我', '？']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '你', '连', '我', '都', '不', '相', '信', '了', '？', '我', '已', '经', '准', '备', '好', '了', '我', '想', '用', '此', '情', '换', '此', '生', '了']\n",
      "['保', '姆', '小', '张', '说', '：', '真', '希', '望', '这', '场', '梦', '没', '有', '醒', '来', '的', '时', '候']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '只', '有', '你', '和', '我', '直', '到', '永', '远']\n",
      "['志', '国', '说', '：', '得', '啦', '和', '平', '折', '腾', '一', '天', '了', '早', '点', '儿', '休', '息', '让', '大', '家', '也', '都', '睡', '吧']\n",
      "['和', '平', '说', '：', '不']\n",
      "['和', '平', '说', '：', '你', '们', '都', '送', '送', '我', '我', '一', '会', '儿', '就', '走']\n",
      "['和', '平', '说', '：', '现', '在', '几', '点', '了']\n",
      "['志', '国', '说', '：', '11', '点', '50']\n",
      "['和', '平', '说', '：', '也', '就', '十', '来', '多', '分', '钟', '了', '我', '知', '道', '我', '熬', '不', '过', '12', '点', '去']\n",
      "['和', '平', '说', '：', '圆', '圆', '来', '妈', '死', '了', '以', '后']\n",
      "['圆', '圆', '说', '：', '哎', '哟', '妈', '你', '怎', '么', '整', '天', '喊', '死', '喊', '活']\n",
      "['圆', '圆', '说', '：', '烦', '死', '我', '了']\n",
      "['傅', '明', '说', '：', '就', '是', '嘛', '好', '好', '的', '一', '个', '大', '活', '人', '自', '己', '说', '死', '就', '死', '了']\n",
      "['傅', '明', '说', '：', '这', '也', '不', '合', '乎', '科', '学', '嘛']\n",
      "['和', '平', '说', '：', '爸', '爸', '唉', '我', '以', '后', '孝', '敬', '不', '了', '您', '啦']\n",
      "['和', '平', '说', '：', '我', '给', '您', '织', '的', '毛', '衣', '还', '差', '一', '领', '口', '您', '找', '别', '人', '帮', '着', '织', '吧']\n",
      "['和', '平', '说', '：', '志', '新']\n",
      "['志', '新', '说', '：', '唉', '在', '在']\n",
      "['和', '平', '说', '：', '来']\n",
      "['和', '平', '说', '：', '往', '后', '你', '多', '照', '顾', '照', '顾', '家', '啊']\n",
      "['志', '新', '说', '：', '嫂', '子', '您', '这', '不', '活', '得', '好', '好', '的', '吗']\n",
      "['志', '新', '说', '：', '怎', '么', '能', '说', '过', '去', '就', '过', '去', '不', '带', '含', '糊', '的', '呀']\n",
      "['和', '平', '说', '：', '小', '凡']\n",
      "['小', '凡', '说', '：', '唉', '来', '了']\n",
      "['和', '平', '说', '：', '来']\n",
      "['和', '平', '说', '：', '我', '第', '一', '放', '心', '不', '下', '圆', '圆', '第', '二', '就', '放', '心', '不', '下', '你']\n",
      "['和', '平', '说', '：', '虽', '说', '你', '是', '妹', '妹', '可', '跟', '我', '闺', '女', '差', '不', '多']\n",
      "['和', '平', '说', '：', '往', '后', '你', '要', '好', '自', '为', '之']\n",
      "['小', '凡', '说', '：', '啊', '说', '得', '跟', '真', '事', '儿', '似', '的']\n",
      "Vocabulary size: 2818\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "        processed_data = []\n",
    "        for line in lines:\n",
    "            json_data = json.loads(line)\n",
    "            processed_data.append(json_data)\n",
    "        self.data = processed_data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    tokens = re.findall(r'[\\u4e00-\\u9fff]', sentence)\n",
    "    return tokens\n",
    "\n",
    "def improved_tokenizer(sentence):\n",
    "    pattern = r'[\\u4e00-\\u9fff]|[0-9]+|[a-zA-Z]+|[^\\u4e00-\\u9fff\\da-zA-Z\\s]'\n",
    "\n",
    "    tokens = re.findall(pattern, sentence)\n",
    "    # tokens = re.findall(r'[\\u4e00-\\u9fff]|\\d+|[a-zA-Z]+|[^\\u4e00-\\u9fff\\da-zA-Z\\s]', sentence)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for item in data_iter:\n",
    "        yield improved_tokenizer(item['sentence'])\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for item in batch:\n",
    "        label_list.append(label_pipeline(item['label'][0]))\n",
    "        token_ids = torch.tensor(text_pipeline(item['sentence']), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0))\n",
    "\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    token_ids = torch.cat(token_ids_list)\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8  \n",
    "train_dataset = TextDataset('train.jsonl')\n",
    "train_iterator = iter(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,collate_fn=collate_batch)\n",
    "test_dataset=TextDataset('test.jsonl')\n",
    "test_dataloader=DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "count = 0\n",
    "for tokens in yield_tokens(train_iterator): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 50:\n",
    "        break\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iterator), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "text_pipeline = lambda x: vocab(improved_tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([8, 2])\n",
      "output: tensor([[-0.0576,  0.0215],\n",
      "        [-0.0584,  0.0416],\n",
      "        [-0.0646,  0.0185],\n",
      "        [-0.0772,  0.0300],\n",
      "        [-0.0675,  0.0210],\n",
      "        [-0.0495,  0.0207],\n",
      "        [-0.0447,  0.0492],\n",
      "        [-0.0559,  0.0845]])\n"
     ]
    }
   ],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim,num_classes):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.embedding_bag = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding_bag(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 64\n",
    "hidden_dim=128\n",
    "num_classes = 2\n",
    "model = BoWClassifier(vocab_size, embed_dim,hidden_dim, num_classes).to(device)\n",
    "\n",
    "EPOCHS = 10  \n",
    "LR = 0.001\n",
    "\n",
    "sparse_parameters = [params for params in model.embedding_bag.parameters()]\n",
    "dense_parameters = [params for params in model.fc.parameters()]\n",
    "optimizer_sparse = optim.SparseAdam(sparse_parameters, lr=LR)\n",
    "optimizer_dense = optim.Adam(dense_parameters, lr=LR)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "sparse_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_sparse, 1.0, gamma=0.1)\n",
    "dense_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_dense, 1.0, gamma=0.1)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "\n",
    "        output = model(token_ids, offsets)\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "# Examine the output\n",
    "print('output size:', output.size())\n",
    "print('output:', output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/ 1268 batches | accuracy    0.684\n",
      "| epoch   1 |   200/ 1268 batches | accuracy    0.711\n",
      "| epoch   1 |   300/ 1268 batches | accuracy    0.706\n",
      "| epoch   1 |   400/ 1268 batches | accuracy    0.708\n",
      "| epoch   1 |   500/ 1268 batches | accuracy    0.694\n",
      "| epoch   1 |   600/ 1268 batches | accuracy    0.701\n",
      "| epoch   1 |   700/ 1268 batches | accuracy    0.743\n",
      "| epoch   1 |   800/ 1268 batches | accuracy    0.718\n",
      "| epoch   1 |   900/ 1268 batches | accuracy    0.711\n",
      "| epoch   1 |  1000/ 1268 batches | accuracy    0.736\n",
      "| epoch   1 |  1100/ 1268 batches | accuracy    0.706\n",
      "| epoch   1 |  1200/ 1268 batches | accuracy    0.729\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  7.12s | accuracy    0.710 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   100/ 1268 batches | accuracy    0.686\n",
      "| epoch   2 |   200/ 1268 batches | accuracy    0.720\n",
      "| epoch   2 |   300/ 1268 batches | accuracy    0.709\n",
      "| epoch   2 |   400/ 1268 batches | accuracy    0.684\n",
      "| epoch   2 |   500/ 1268 batches | accuracy    0.724\n",
      "| epoch   2 |   600/ 1268 batches | accuracy    0.718\n",
      "| epoch   2 |   700/ 1268 batches | accuracy    0.720\n",
      "| epoch   2 |   800/ 1268 batches | accuracy    0.730\n",
      "| epoch   2 |   900/ 1268 batches | accuracy    0.724\n",
      "| epoch   2 |  1000/ 1268 batches | accuracy    0.744\n",
      "| epoch   2 |  1100/ 1268 batches | accuracy    0.725\n",
      "| epoch   2 |  1200/ 1268 batches | accuracy    0.710\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  8.54s | accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   100/ 1268 batches | accuracy    0.746\n",
      "| epoch   3 |   200/ 1268 batches | accuracy    0.720\n",
      "| epoch   3 |   300/ 1268 batches | accuracy    0.748\n",
      "| epoch   3 |   400/ 1268 batches | accuracy    0.726\n",
      "| epoch   3 |   500/ 1268 batches | accuracy    0.714\n",
      "| epoch   3 |   600/ 1268 batches | accuracy    0.726\n",
      "| epoch   3 |   700/ 1268 batches | accuracy    0.740\n",
      "| epoch   3 |   800/ 1268 batches | accuracy    0.726\n",
      "| epoch   3 |   900/ 1268 batches | accuracy    0.705\n",
      "| epoch   3 |  1000/ 1268 batches | accuracy    0.736\n",
      "| epoch   3 |  1100/ 1268 batches | accuracy    0.734\n",
      "| epoch   3 |  1200/ 1268 batches | accuracy    0.739\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  6.98s | accuracy    0.705 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   100/ 1268 batches | accuracy    0.762\n",
      "| epoch   4 |   200/ 1268 batches | accuracy    0.741\n",
      "| epoch   4 |   300/ 1268 batches | accuracy    0.745\n",
      "| epoch   4 |   400/ 1268 batches | accuracy    0.754\n",
      "| epoch   4 |   500/ 1268 batches | accuracy    0.772\n",
      "| epoch   4 |   600/ 1268 batches | accuracy    0.765\n",
      "| epoch   4 |   700/ 1268 batches | accuracy    0.749\n",
      "| epoch   4 |   800/ 1268 batches | accuracy    0.785\n",
      "| epoch   4 |   900/ 1268 batches | accuracy    0.759\n",
      "| epoch   4 |  1000/ 1268 batches | accuracy    0.770\n",
      "| epoch   4 |  1100/ 1268 batches | accuracy    0.780\n",
      "| epoch   4 |  1200/ 1268 batches | accuracy    0.757\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  7.58s | accuracy    0.702 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   100/ 1268 batches | accuracy    0.756\n",
      "| epoch   5 |   200/ 1268 batches | accuracy    0.779\n",
      "| epoch   5 |   300/ 1268 batches | accuracy    0.757\n",
      "| epoch   5 |   400/ 1268 batches | accuracy    0.738\n",
      "| epoch   5 |   500/ 1268 batches | accuracy    0.775\n",
      "| epoch   5 |   600/ 1268 batches | accuracy    0.767\n",
      "| epoch   5 |   700/ 1268 batches | accuracy    0.797\n",
      "| epoch   5 |   800/ 1268 batches | accuracy    0.794\n",
      "| epoch   5 |   900/ 1268 batches | accuracy    0.765\n",
      "| epoch   5 |  1000/ 1268 batches | accuracy    0.762\n",
      "| epoch   5 |  1100/ 1268 batches | accuracy    0.765\n",
      "| epoch   5 |  1200/ 1268 batches | accuracy    0.775\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  6.60s | accuracy    0.703 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   100/ 1268 batches | accuracy    0.766\n",
      "| epoch   6 |   200/ 1268 batches | accuracy    0.764\n",
      "| epoch   6 |   300/ 1268 batches | accuracy    0.774\n",
      "| epoch   6 |   400/ 1268 batches | accuracy    0.770\n",
      "| epoch   6 |   500/ 1268 batches | accuracy    0.756\n",
      "| epoch   6 |   600/ 1268 batches | accuracy    0.750\n",
      "| epoch   6 |   700/ 1268 batches | accuracy    0.786\n",
      "| epoch   6 |   800/ 1268 batches | accuracy    0.746\n",
      "| epoch   6 |   900/ 1268 batches | accuracy    0.782\n",
      "| epoch   6 |  1000/ 1268 batches | accuracy    0.771\n",
      "| epoch   6 |  1100/ 1268 batches | accuracy    0.765\n",
      "| epoch   6 |  1200/ 1268 batches | accuracy    0.787\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  6.99s | accuracy    0.703 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   100/ 1268 batches | accuracy    0.769\n",
      "| epoch   7 |   200/ 1268 batches | accuracy    0.746\n",
      "| epoch   7 |   300/ 1268 batches | accuracy    0.766\n",
      "| epoch   7 |   400/ 1268 batches | accuracy    0.790\n",
      "| epoch   7 |   500/ 1268 batches | accuracy    0.801\n",
      "| epoch   7 |   600/ 1268 batches | accuracy    0.767\n",
      "| epoch   7 |   700/ 1268 batches | accuracy    0.787\n",
      "| epoch   7 |   800/ 1268 batches | accuracy    0.770\n",
      "| epoch   7 |   900/ 1268 batches | accuracy    0.759\n",
      "| epoch   7 |  1000/ 1268 batches | accuracy    0.779\n",
      "| epoch   7 |  1100/ 1268 batches | accuracy    0.745\n",
      "| epoch   7 |  1200/ 1268 batches | accuracy    0.780\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  5.82s | accuracy    0.703 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   100/ 1268 batches | accuracy    0.751\n",
      "| epoch   8 |   200/ 1268 batches | accuracy    0.785\n",
      "| epoch   8 |   300/ 1268 batches | accuracy    0.775\n",
      "| epoch   8 |   400/ 1268 batches | accuracy    0.787\n",
      "| epoch   8 |   500/ 1268 batches | accuracy    0.762\n",
      "| epoch   8 |   600/ 1268 batches | accuracy    0.765\n",
      "| epoch   8 |   700/ 1268 batches | accuracy    0.795\n",
      "| epoch   8 |   800/ 1268 batches | accuracy    0.762\n",
      "| epoch   8 |   900/ 1268 batches | accuracy    0.750\n",
      "| epoch   8 |  1000/ 1268 batches | accuracy    0.756\n",
      "| epoch   8 |  1100/ 1268 batches | accuracy    0.777\n",
      "| epoch   8 |  1200/ 1268 batches | accuracy    0.779\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  5.10s | accuracy    0.703 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   100/ 1268 batches | accuracy    0.785\n",
      "| epoch   9 |   200/ 1268 batches | accuracy    0.766\n",
      "| epoch   9 |   300/ 1268 batches | accuracy    0.777\n",
      "| epoch   9 |   400/ 1268 batches | accuracy    0.751\n",
      "| epoch   9 |   500/ 1268 batches | accuracy    0.775\n",
      "| epoch   9 |   600/ 1268 batches | accuracy    0.759\n",
      "| epoch   9 |   700/ 1268 batches | accuracy    0.774\n",
      "| epoch   9 |   800/ 1268 batches | accuracy    0.786\n",
      "| epoch   9 |   900/ 1268 batches | accuracy    0.762\n",
      "| epoch   9 |  1000/ 1268 batches | accuracy    0.772\n",
      "| epoch   9 |  1100/ 1268 batches | accuracy    0.745\n",
      "| epoch   9 |  1200/ 1268 batches | accuracy    0.791\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  6.21s | accuracy    0.703 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   100/ 1268 batches | accuracy    0.790\n",
      "| epoch  10 |   200/ 1268 batches | accuracy    0.740\n",
      "| epoch  10 |   300/ 1268 batches | accuracy    0.760\n",
      "| epoch  10 |   400/ 1268 batches | accuracy    0.775\n",
      "| epoch  10 |   500/ 1268 batches | accuracy    0.795\n",
      "| epoch  10 |   600/ 1268 batches | accuracy    0.785\n",
      "| epoch  10 |   700/ 1268 batches | accuracy    0.767\n",
      "| epoch  10 |   800/ 1268 batches | accuracy    0.802\n",
      "| epoch  10 |   900/ 1268 batches | accuracy    0.766\n",
      "| epoch  10 |  1000/ 1268 batches | accuracy    0.741\n",
      "| epoch  10 |  1100/ 1268 batches | accuracy    0.757\n",
      "| epoch  10 |  1200/ 1268 batches | accuracy    0.772\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  6.27s | accuracy    0.703 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def eval(model, dataloader,criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, label)\n",
    "            total_acc += (output.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_dataloader,optimizer_sparse,optimizer_dense, criterion, epoch: int):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 100\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (labels, tokens, offsets) in enumerate(train_dataloader):\n",
    "        optimizer_sparse.zero_grad()\n",
    "        optimizer_dense.zero_grad()\n",
    "        output = model(tokens, offsets)\n",
    "        try:\n",
    "            loss = criterion(output, labels)\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            # print('token_ids: ', token_ids)\n",
    "            # print('offsets: ', offsets)\n",
    "            raise\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer_sparse.step()\n",
    "        optimizer_dense.step()\n",
    "\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(train_dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.8)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "# Run the training loop\n",
    "total_accu = None\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_dataloader,optimizer_sparse,optimizer_dense, criterion, epoch)\n",
    "    accu_val = eval(model, valid_dataloader,criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu >= accu_val:\n",
    "        sparse_scheduler.step()\n",
    "        dense_scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72657450\n",
      "Precision: 0.45000000\n",
      "Recall: 0.21176471\n",
      "F1 Score: 0.28800000\n"
     ]
    }
   ],
   "source": [
    "model = BoWClassifier(vocab_size, embed_dim, hidden_dim,num_classes)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "def result_evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (labels, text, offsets) in enumerate(dataloader):\n",
    "            output = model(text, offsets)\n",
    "            predictions = output.argmax(dim=1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    precision = precision_score(all_true_labels, all_predictions, average='binary', zero_division=0)\n",
    "    recall = recall_score(all_true_labels, all_predictions, average='binary', zero_division=0)\n",
    "    f1 = f1_score(all_true_labels, all_predictions, average='binary', zero_division=0)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy, precision, recall, f1 = result_evaluate(model, test_dataloader, criterion)\n",
    "print(f\"Accuracy: {accuracy:.8f}\")\n",
    "print(f\"Precision: {precision:.8f}\")\n",
    "print(f\"Recall: {recall:.8f}\")\n",
    "print(f\"F1 Score: {f1:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/qd/pr_ybl6s7c7d3nbyj6yt9g0w0000gn/T/jieba.cache\n",
      "Loading model cost 1.083 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['卖', '油条', '小', '刘说', '：', '我', '说']\n",
      "['保姆', '小张', '说', '：', '干', '啥子', '嘛', '？']\n",
      "['卖', '油条', '小', '刘说', '：', '你', '看', '你', '往', '星空', '看', '月', '朦胧', '，', '鸟', '朦胧']\n",
      "['卖', '油条', '小', '刘说', '：', '咱', '是不是', '歇', '一下', '这', '双', '，', '疲惫', '的', '双腿', '？']\n",
      "['卖', '油条', '小', '刘说', '：', '快', '把', '我', '累死', '了']\n",
      "['卖', '油条', '小', '刘说', '：', '我', '说', '亲爱', '的', '大姐', '你', '贵姓', '啊', '？']\n",
      "['保姆', '小张', '说', '：', '我免', '贵姓', '张', '我', '叫', '张凤姑']\n",
      "['卖', '油条', '小', '刘说', '：', '凤姑']\n",
      "['保姆', '小张', '说', '：', '天天', '买', '你', '的', '油条', '还', '没有', '问过', '师傅', '，', '你', '贵姓', '啊', '？']\n",
      "['卖', '油条', '小', '刘说', '：', '我免', '贵', '，', '我姓', '刘', '，', '我', '叫', '刘建军']\n",
      "['卖', '油条', '小', '刘说', '：', '凤', '姑姑']\n",
      "['卖', '油条', '小', '刘说', '：', '我', '的', '姑', '啊', '我', '亲爱', '的', '姑']\n",
      "['卖', '油条', '小', '刘说', '：', '我', '怎么', '那么', '别扭', '呢', '？']\n",
      "['卖', '油条', '小', '刘说', '：', '我', '自从', '见', '了', '你', '以后', '我', '的', '这个', '生活', '，', '我', '的', '这个', '事业', '发生', '了', '翻天覆地', '的', '变化']\n",
      "['卖', '油条', '小', '刘说', '：', '只要', '你', '的', '身影', '，', '从', '那个', '胡同口', '噌', '一', '出现']\n",
      "['卖', '油条', '小', '刘说', '：', '我', '的', '这个', '心里', '头', '就', '像', '那个', '油锅', '一样', '咕噜噜', '，', '咕噜噜', '，', '热血沸腾']\n",
      "['卖', '油条', '小', '刘说', '：', '我', '跟', '你', '说', '吧', '咱们', '两个', '是', '同样', '的', '理想', '，', '同样', '的', '心']\n",
      "['卖', '油条', '小', '刘说', '：', '同是天涯沦落人', '天下', '农友', '心连心', '穷不帮', '穷', '谁', '照应', '？']\n",
      "['卖', '油条', '小', '刘说', '：', '我', '都', '说了半天', '了', '，', '你', '倒', '说话', '呀']\n",
      "['保姆', '小张', '说', '：', '我', '是', '在', '用', '不在乎', '，', '掩藏', '真心']\n",
      "['卖', '油条', '小', '刘说', '：', '我', '这', '就', '想', '抓起', '你', '的', '手', '让', '你', '这', '就', '跟我走']\n",
      "['保姆', '小张', '说', '：', '莫慌', '喽', '谁', '知道', '你', '明天', '是否', '依然', '爱', '我', '？']\n",
      "['卖', '油条', '小', '刘说', '：', '你', '连', '我', '都', '不', '相信', '了', '？', '我', '已经', '准备', '好', '了', '我', '想', '用', '此情', '换', '此生', '了']\n",
      "['保姆', '小张', '说', '：', '真', '希望', '这场', '梦', '没有', '醒来', '的', '时候']\n",
      "['卖', '油条', '小', '刘说', '：', '只有', '你', '和', '我', '直到', '永远']\n",
      "['志国', '说', '：', '得', '啦', '和平', '折腾', '一天', '了', '早点儿', '休息', '让', '大家', '也', '都', '睡', '吧']\n",
      "['和平', '说', '：', '不']\n",
      "['和平', '说', '：', '你们', '都', '送', '送', '我', '我', '一会儿', '就', '走']\n",
      "['和平', '说', '：', '现在', '几点', '了']\n",
      "['志国', '说', '：', '11', '点', '50']\n",
      "['和平', '说', '：', '也', '就', '十来', '多分钟', '了', '我', '知道', '我', '熬不过', '12', '点去']\n",
      "['和平', '说', '：', '圆圆', '来妈', '死', '了', '以后']\n",
      "['圆圆', '说', '：', '哎哟', '妈', '你', '怎么', '整天', '喊', '死', '喊活']\n",
      "['圆圆', '说', '：', '烦死', '我', '了']\n",
      "['傅', '明说', '：', '就是', '嘛', '好好', '的', '一个', '大', '活人', '自己', '说', '死就死', '了']\n",
      "['傅', '明说', '：', '这', '也', '不', '合乎', '科学', '嘛']\n",
      "['和平', '说', '：', '爸爸', '唉', '我', '以后', '孝敬', '不了', '您', '啦']\n",
      "['和平', '说', '：', '我', '给', '您', '织', '的', '毛衣', '还', '差', '一', '领口', '您', '找', '别人', '帮着织', '吧']\n",
      "['和平', '说', '：', '志新']\n",
      "['志新', '说', '：', '唉', ' ', ' ', '在', '在']\n",
      "['和平', '说', '：', '来']\n",
      "['和平', '说', '：', '往后', '你', '多', '照顾', '照顾', '家', ' ', ' ', '啊']\n",
      "['志新', '说', '：', '嫂子', '您', '这', '不活', '得', '好好', '的', '吗']\n",
      "['志新', '说', '：', '怎么', '能', '说', '过去', '就', '过去', '不带', '含糊', '的', '呀']\n",
      "['和平', '说', '：', '小凡']\n",
      "['小凡', '说', '：', '唉', ' ', '来', '了']\n",
      "['和平', '说', '：', '来']\n",
      "['和平', '说', '：', '我', '第一', '放心不下', '圆圆', '第二', '就', '放心不下', '你']\n",
      "['和平', '说', '：', '虽说', '你', '是', '妹妹', '可', '跟', '我', '闺女', '差不多']\n",
      "['和平', '说', '：', '往后', '你', '要', '好自为之']\n",
      "['小凡', '说', '：', '啊', '说', '得', '跟', '真', '事儿', '似的']\n",
      "Vocabulary size: 13817\n"
     ]
    }
   ],
   "source": [
    "def jieba_tokenizer(sentence):\n",
    "    tokens=[]\n",
    "    seg_list = jieba.cut(sentence)\n",
    "    for seg in seg_list:\n",
    "        tokens.append(seg)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for item in data_iter:\n",
    "        yield jieba_tokenizer(item['sentence'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8  \n",
    "train_dataset = TextDataset('train.jsonl')\n",
    "train_iterator = iter(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,collate_fn=collate_batch)\n",
    "test_dataset=TextDataset('test.jsonl')\n",
    "test_dataloader=DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "count = 0\n",
    "for tokens in yield_tokens(train_iterator): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 50:\n",
    "        break\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iterator), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "text_pipeline = lambda x: vocab(jieba_tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([8, 2])\n",
      "output: tensor([[-0.0821, -0.0335],\n",
      "        [-0.0669,  0.0211],\n",
      "        [-0.0743, -0.0733],\n",
      "        [-0.0807,  0.0118],\n",
      "        [-0.0721,  0.0080],\n",
      "        [-0.0567,  0.0034],\n",
      "        [-0.1384,  0.0338],\n",
      "        [-0.0365, -0.0220]])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 64\n",
    "hidden_dim=128\n",
    "num_classes = 2\n",
    "model = BoWClassifier(vocab_size, embed_dim,hidden_dim, num_classes).to(device)\n",
    "EPOCHS = 10  \n",
    "LR = 0.001\n",
    "\n",
    "sparse_parameters = [params for params in model.embedding_bag.parameters()]\n",
    "dense_parameters = [params for params in model.fc.parameters()]\n",
    "optimizer_sparse = optim.SparseAdam(sparse_parameters, lr=LR)\n",
    "optimizer_dense = optim.Adam(dense_parameters, lr=LR)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "sparse_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_sparse, 1.0, gamma=0.1)\n",
    "dense_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_dense, 1.0, gamma=0.1)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "\n",
    "        output = model(token_ids, offsets)\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "# Examine the output\n",
    "print('output size:', output.size())\n",
    "print('output:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/ 1268 batches | accuracy    0.707\n",
      "| epoch   1 |   200/ 1268 batches | accuracy    0.705\n",
      "| epoch   1 |   300/ 1268 batches | accuracy    0.699\n",
      "| epoch   1 |   400/ 1268 batches | accuracy    0.708\n",
      "| epoch   1 |   500/ 1268 batches | accuracy    0.700\n",
      "| epoch   1 |   600/ 1268 batches | accuracy    0.713\n",
      "| epoch   1 |   700/ 1268 batches | accuracy    0.721\n",
      "| epoch   1 |   800/ 1268 batches | accuracy    0.700\n",
      "| epoch   1 |   900/ 1268 batches | accuracy    0.708\n",
      "| epoch   1 |  1000/ 1268 batches | accuracy    0.710\n",
      "| epoch   1 |  1100/ 1268 batches | accuracy    0.699\n",
      "| epoch   1 |  1200/ 1268 batches | accuracy    0.738\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  8.85s | accuracy    0.723 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   100/ 1268 batches | accuracy    0.703\n",
      "| epoch   2 |   200/ 1268 batches | accuracy    0.726\n",
      "| epoch   2 |   300/ 1268 batches | accuracy    0.714\n",
      "| epoch   2 |   400/ 1268 batches | accuracy    0.721\n",
      "| epoch   2 |   500/ 1268 batches | accuracy    0.710\n",
      "| epoch   2 |   600/ 1268 batches | accuracy    0.728\n",
      "| epoch   2 |   700/ 1268 batches | accuracy    0.730\n",
      "| epoch   2 |   800/ 1268 batches | accuracy    0.699\n",
      "| epoch   2 |   900/ 1268 batches | accuracy    0.710\n",
      "| epoch   2 |  1000/ 1268 batches | accuracy    0.705\n",
      "| epoch   2 |  1100/ 1268 batches | accuracy    0.724\n",
      "| epoch   2 |  1200/ 1268 batches | accuracy    0.718\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  8.35s | accuracy    0.724 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   100/ 1268 batches | accuracy    0.746\n",
      "| epoch   3 |   200/ 1268 batches | accuracy    0.721\n",
      "| epoch   3 |   300/ 1268 batches | accuracy    0.746\n",
      "| epoch   3 |   400/ 1268 batches | accuracy    0.721\n",
      "| epoch   3 |   500/ 1268 batches | accuracy    0.709\n",
      "| epoch   3 |   600/ 1268 batches | accuracy    0.766\n",
      "| epoch   3 |   700/ 1268 batches | accuracy    0.709\n",
      "| epoch   3 |   800/ 1268 batches | accuracy    0.733\n",
      "| epoch   3 |   900/ 1268 batches | accuracy    0.713\n",
      "| epoch   3 |  1000/ 1268 batches | accuracy    0.719\n",
      "| epoch   3 |  1100/ 1268 batches | accuracy    0.703\n",
      "| epoch   3 |  1200/ 1268 batches | accuracy    0.750\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  8.49s | accuracy    0.700 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   100/ 1268 batches | accuracy    0.745\n",
      "| epoch   4 |   200/ 1268 batches | accuracy    0.776\n",
      "| epoch   4 |   300/ 1268 batches | accuracy    0.769\n",
      "| epoch   4 |   400/ 1268 batches | accuracy    0.764\n",
      "| epoch   4 |   500/ 1268 batches | accuracy    0.745\n",
      "| epoch   4 |   600/ 1268 batches | accuracy    0.750\n",
      "| epoch   4 |   700/ 1268 batches | accuracy    0.744\n",
      "| epoch   4 |   800/ 1268 batches | accuracy    0.776\n",
      "| epoch   4 |   900/ 1268 batches | accuracy    0.774\n",
      "| epoch   4 |  1000/ 1268 batches | accuracy    0.752\n",
      "| epoch   4 |  1100/ 1268 batches | accuracy    0.769\n",
      "| epoch   4 |  1200/ 1268 batches | accuracy    0.756\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  9.68s | accuracy    0.689 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   100/ 1268 batches | accuracy    0.788\n",
      "| epoch   5 |   200/ 1268 batches | accuracy    0.775\n",
      "| epoch   5 |   300/ 1268 batches | accuracy    0.762\n",
      "| epoch   5 |   400/ 1268 batches | accuracy    0.786\n",
      "| epoch   5 |   500/ 1268 batches | accuracy    0.762\n",
      "| epoch   5 |   600/ 1268 batches | accuracy    0.741\n",
      "| epoch   5 |   700/ 1268 batches | accuracy    0.766\n",
      "| epoch   5 |   800/ 1268 batches | accuracy    0.800\n",
      "| epoch   5 |   900/ 1268 batches | accuracy    0.741\n",
      "| epoch   5 |  1000/ 1268 batches | accuracy    0.756\n",
      "| epoch   5 |  1100/ 1268 batches | accuracy    0.754\n",
      "| epoch   5 |  1200/ 1268 batches | accuracy    0.752\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  9.20s | accuracy    0.690 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   100/ 1268 batches | accuracy    0.750\n",
      "| epoch   6 |   200/ 1268 batches | accuracy    0.756\n",
      "| epoch   6 |   300/ 1268 batches | accuracy    0.755\n",
      "| epoch   6 |   400/ 1268 batches | accuracy    0.777\n",
      "| epoch   6 |   500/ 1268 batches | accuracy    0.765\n",
      "| epoch   6 |   600/ 1268 batches | accuracy    0.755\n",
      "| epoch   6 |   700/ 1268 batches | accuracy    0.786\n",
      "| epoch   6 |   800/ 1268 batches | accuracy    0.774\n",
      "| epoch   6 |   900/ 1268 batches | accuracy    0.775\n",
      "| epoch   6 |  1000/ 1268 batches | accuracy    0.781\n",
      "| epoch   6 |  1100/ 1268 batches | accuracy    0.759\n",
      "| epoch   6 |  1200/ 1268 batches | accuracy    0.772\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  8.04s | accuracy    0.690 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   100/ 1268 batches | accuracy    0.787\n",
      "| epoch   7 |   200/ 1268 batches | accuracy    0.751\n",
      "| epoch   7 |   300/ 1268 batches | accuracy    0.777\n",
      "| epoch   7 |   400/ 1268 batches | accuracy    0.774\n",
      "| epoch   7 |   500/ 1268 batches | accuracy    0.776\n",
      "| epoch   7 |   600/ 1268 batches | accuracy    0.762\n",
      "| epoch   7 |   700/ 1268 batches | accuracy    0.771\n",
      "| epoch   7 |   800/ 1268 batches | accuracy    0.750\n",
      "| epoch   7 |   900/ 1268 batches | accuracy    0.739\n",
      "| epoch   7 |  1000/ 1268 batches | accuracy    0.775\n",
      "| epoch   7 |  1100/ 1268 batches | accuracy    0.772\n",
      "| epoch   7 |  1200/ 1268 batches | accuracy    0.767\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  7.99s | accuracy    0.690 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   100/ 1268 batches | accuracy    0.767\n",
      "| epoch   8 |   200/ 1268 batches | accuracy    0.775\n",
      "| epoch   8 |   300/ 1268 batches | accuracy    0.785\n",
      "| epoch   8 |   400/ 1268 batches | accuracy    0.781\n",
      "| epoch   8 |   500/ 1268 batches | accuracy    0.776\n",
      "| epoch   8 |   600/ 1268 batches | accuracy    0.772\n",
      "| epoch   8 |   700/ 1268 batches | accuracy    0.713\n",
      "| epoch   8 |   800/ 1268 batches | accuracy    0.775\n",
      "| epoch   8 |   900/ 1268 batches | accuracy    0.740\n",
      "| epoch   8 |  1000/ 1268 batches | accuracy    0.772\n",
      "| epoch   8 |  1100/ 1268 batches | accuracy    0.751\n",
      "| epoch   8 |  1200/ 1268 batches | accuracy    0.791\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  6.80s | accuracy    0.690 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   100/ 1268 batches | accuracy    0.761\n",
      "| epoch   9 |   200/ 1268 batches | accuracy    0.756\n",
      "| epoch   9 |   300/ 1268 batches | accuracy    0.752\n",
      "| epoch   9 |   400/ 1268 batches | accuracy    0.757\n",
      "| epoch   9 |   500/ 1268 batches | accuracy    0.792\n",
      "| epoch   9 |   600/ 1268 batches | accuracy    0.775\n",
      "| epoch   9 |   700/ 1268 batches | accuracy    0.759\n",
      "| epoch   9 |   800/ 1268 batches | accuracy    0.770\n",
      "| epoch   9 |   900/ 1268 batches | accuracy    0.764\n",
      "| epoch   9 |  1000/ 1268 batches | accuracy    0.757\n",
      "| epoch   9 |  1100/ 1268 batches | accuracy    0.767\n",
      "| epoch   9 |  1200/ 1268 batches | accuracy    0.765\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  9.01s | accuracy    0.690 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   100/ 1268 batches | accuracy    0.788\n",
      "| epoch  10 |   200/ 1268 batches | accuracy    0.761\n",
      "| epoch  10 |   300/ 1268 batches | accuracy    0.782\n",
      "| epoch  10 |   400/ 1268 batches | accuracy    0.754\n",
      "| epoch  10 |   500/ 1268 batches | accuracy    0.757\n",
      "| epoch  10 |   600/ 1268 batches | accuracy    0.774\n",
      "| epoch  10 |   700/ 1268 batches | accuracy    0.751\n",
      "| epoch  10 |   800/ 1268 batches | accuracy    0.774\n",
      "| epoch  10 |   900/ 1268 batches | accuracy    0.751\n",
      "| epoch  10 |  1000/ 1268 batches | accuracy    0.762\n",
      "| epoch  10 |  1100/ 1268 batches | accuracy    0.779\n",
      "| epoch  10 |  1200/ 1268 batches | accuracy    0.765\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  7.75s | accuracy    0.690 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_train = int(len(train_dataset) * 0.8)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "\n",
    "# Run the training loop\n",
    "total_accu = None\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_dataloader,optimizer_sparse,optimizer_dense, criterion, epoch)\n",
    "    accu_val = eval(model, valid_dataloader,criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu >= accu_val:\n",
    "        sparse_scheduler.step()\n",
    "        dense_scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "torch.save(model.state_dict(), 'model_jieba.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71735791\n",
      "Precision: 0.44696970\n",
      "Recall: 0.34705882\n",
      "F1 Score: 0.39072848\n"
     ]
    }
   ],
   "source": [
    "model = BoWClassifier(vocab_size, embed_dim,hidden_dim, num_classes)\n",
    "model.load_state_dict(torch.load('model_jieba.pth'))\n",
    "\n",
    "# 使用函数\n",
    "accuracy, precision, recall, f1 = result_evaluate(model, test_dataloader, criterion)\n",
    "print(f\"Accuracy: {accuracy:.8f}\")\n",
    "print(f\"Precision: {precision:.8f}\")\n",
    "print(f\"Recall: {recall:.8f}\")\n",
    "print(f\"F1 Score: {f1:.8f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 4. Dependency Parsing\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "In this assignment, you will train feed-forward neural network-based dependency parser and evaluate its performance on the provided treebank dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from dep_utils import conll_reader, DependencyTree\n",
    "import copy\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Data and Generate Training Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train.conll:\n",
      "39712 trees read.\n",
      "In dev.conll:\n",
      "1695 trees read.\n",
      "In test.conll:\n",
      "2408 trees read.\n"
     ]
    }
   ],
   "source": [
    "print('In train.conll:')\n",
    "with open('valid_data/valid_train.conll') as f:\n",
    "    train_trees = list(conll_reader(f))\n",
    "print(f'{len(train_trees)} trees read.')\n",
    "\n",
    "print('In dev.conll:')\n",
    "with open('valid_data/valid_dev.conll') as f:\n",
    "    dev_trees = list(conll_reader(f))\n",
    "print(f'{len(dev_trees)} trees read.')\n",
    "\n",
    "print('In test.conll:')\n",
    "with open('valid_data/valid_test.conll') as f:\n",
    "    test_trees = list(conll_reader(f))\n",
    "print(f'{len(test_trees)} trees read.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use the code from Lab 7\n",
    "class RootDummy(object):\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.id = 0\n",
    "        self.deprel = None\n",
    "    def __repr__(self):\n",
    "        return \"<ROOT>\"\n",
    "\n",
    "class State(object):\n",
    "    def __init__(self, sentence=[],words=[]):\n",
    "        self.stack = []\n",
    "        self.buffer = []\n",
    "        self.stack_words = []\n",
    "        self.buffer_words = []\n",
    "        if sentence:\n",
    "            self.buffer = list(reversed(sentence))\n",
    "        if words:\n",
    "            self.buffer_words = list(reversed(words))\n",
    "        self.deps = set()\n",
    "\n",
    "    def shift(self):\n",
    "        ### START YOUR CODE ###\n",
    "        if self.buffer:\n",
    "            front_of_buffer=self.buffer[-1]\n",
    "            self.buffer.pop()\n",
    "            self.stack.append(front_of_buffer)\n",
    "\n",
    "            front_of_buffer_word=self.buffer_words[-1]\n",
    "            self.buffer_words.pop()\n",
    "            self.stack_words.append(front_of_buffer_word)\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def left_arc(self, label: str):\n",
    "        assert len(self.stack) >= 2\n",
    "        ### START YOUR CODE ###\n",
    "\n",
    "        s1=self.stack[-1]\n",
    "        s2 = self.stack[-2]  \n",
    "\n",
    "        s1_word=self.stack_words[-1]\n",
    "        s2_word = self.stack_words[-2]  \n",
    "        \n",
    "\n",
    "        self.stack.pop(-2)\n",
    "        self.stack_words.pop(-2)\n",
    "\n",
    "        self.deps.add((s1, s2, label))\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def right_arc(self, label: str):\n",
    "        assert len(self.stack) >= 2\n",
    "        ### START YOUR CODE ###\n",
    "        s1=self.stack[-1]\n",
    "        s2 = self.stack[-2] \n",
    "\n",
    "        s1_word=self.stack_words[-1]\n",
    "        s2_word = self.stack_words[-2] \n",
    "        \n",
    "        self.stack.pop()\n",
    "        self.stack_words.pop()\n",
    "\n",
    "        self.deps.add((s2, s1, label))\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"({},{},{},{},{})\".format(self.stack, self.buffer, self.deps,self.stack_words,self.buffer_words)\n",
    "\n",
    "def get_training_instances(dep_tree) -> List[Tuple[State, Tuple[str, str]]]:\n",
    "    deprels = dep_tree.deprels\n",
    "\n",
    "    word_ids = list(deprels.keys())\n",
    "    words_list=[]\n",
    "    words = list(deprels.values())\n",
    "    for i in words:\n",
    "        words_list.append((i.word,i.pos))\n",
    "    state = State(word_ids,words_list)\n",
    "    state.stack.append(0) # ROOT\n",
    "    state.stack_words.append((\"<ROOT>\",'<ROOT>')) # ROOT\n",
    "\n",
    "    childcount = defaultdict(int)\n",
    "    for _, rel in deprels.items():\n",
    "        childcount[rel.head] += 1\n",
    "\n",
    "    seq = []\n",
    "\n",
    "\n",
    "    dep_relation=[]\n",
    "    for i in range(1,len(deprels)+1):\n",
    "        dep_relation.append((deprels[i].head,i))\n",
    "    \n",
    "\n",
    "    while len(state.buffer) > 0 or len(state.stack) > 1:\n",
    "    \n",
    "\n",
    "        if state.stack[-1] == 0:\n",
    "            seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "            state.shift()\n",
    "            continue\n",
    "        \n",
    "        stack_top1 = deprels[state.stack[-1]]\n",
    "        if state.stack[-2] == 0:\n",
    "            stack_top2 = RootDummy()\n",
    "        else:\n",
    "            stack_top2 = deprels[state.stack[-2]]\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "\n",
    "        if (int(state.stack[-1]), int(state.stack[-2])) in dep_relation:\n",
    "            relation = stack_top2.deprel\n",
    "            \n",
    "            action = \"left_arc\"\n",
    "            \n",
    "            seq.append((copy.deepcopy(state), (action, relation)))\n",
    "            childcount[state.stack[-1]] -= 1 \n",
    "            state.left_arc(relation)\n",
    "        elif (int(state.stack[-2]), int(state.stack[-1])) in dep_relation and childcount[state.stack[-1]] == 0:\n",
    "            relation = stack_top1.deprel\n",
    "            \n",
    "            action = \"right_arc\"\n",
    "            \n",
    "            seq.append((copy.deepcopy(state), (action, relation)))\n",
    "            childcount[state.stack[-2]] -= 1\n",
    "            state.right_arc(relation)\n",
    "        else:\n",
    "            \n",
    "            seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "            state.shift()\n",
    "            \n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "    seq.append((copy.deepcopy(state), (\"done\", None)))\n",
    "\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44307\n",
      "46\n",
      "39\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_set=[]\n",
    "tags_set=[]\n",
    "actions_set=[]\n",
    "for tree in train_trees:\n",
    "    words_set.extend(tree.words())\n",
    "    tags_set.extend(tree.pos())\n",
    "    \n",
    "    for i in tree.deprels:\n",
    "        actions_set.append(tree.deprels[i].deprel)\n",
    "\n",
    "words_set=set(words_set)\n",
    "tags_set=set(tags_set)\n",
    "actions_set=set(actions_set)\n",
    "\n",
    "\n",
    "def create_action_vocab():\n",
    "    action_vocab=[]\n",
    "\n",
    "    for i in actions_set:\n",
    "        if i !='root':\n",
    "            action_vocab.append(('left_arc',i))\n",
    "            action_vocab.append(('right_arc',i))\n",
    "        else:\n",
    "            action_vocab.append(('right_arc',i))\n",
    "\n",
    "    \n",
    "    action_vocab.append(('shift', None))\n",
    "    \n",
    "    return action_vocab\n",
    "\n",
    "action_vocab=create_action_vocab()\n",
    "print(len(words_set))\n",
    "print(len(tags_set))\n",
    "print(len(actions_set))\n",
    "print(len(action_vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Implement the Feature Extractor (10 points) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION=100\n",
    "NUM_FEATURES = 12  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class BagOfWordsEmbedding:\n",
    "    def __init__(self, dimension):\n",
    "        self.vocab = {}\n",
    "        self.vocab_size = 0\n",
    "        self.dimension = dimension\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        \"\"\"Build vocabulary.\"\"\"\n",
    "        word_counts = Counter()\n",
    "        for document in corpus:\n",
    "            if document is not None:\n",
    "                word_counts.update(document.split())\n",
    "        self.vocab = {'<NULL>': 0, '<ROOT>': 1}\n",
    "        idx = 2\n",
    "        for word, _ in word_counts.items():\n",
    "            self.vocab[word] = idx\n",
    "            idx += 1\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def text_to_bow_vector(self, text):\n",
    "        \"\"\"Convert text to a bag-of-words vector.\"\"\"\n",
    "        if text is None:\n",
    "            return np.zeros(self.dimension)\n",
    "        bow_vector = np.zeros(self.dimension)\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if word in self.vocab:\n",
    "                bow_vector[self.vocab[word] % self.dimension] += 1\n",
    "        return bow_vector\n",
    "\n",
    "\n",
    "\n",
    "words_embedding = BagOfWordsEmbedding(DIMENSION)\n",
    "words_embedding.build_vocab(words_set)\n",
    "tags_embedding = BagOfWordsEmbedding(DIMENSION)\n",
    "tags_embedding.build_vocab(tags_set)\n",
    "\n",
    "\n",
    "word = \"<ROOT>\"\n",
    "embedding1 = words_embedding.text_to_bow_vector(word)\n",
    "print(embedding1)\n",
    "word = \"<NULL>\"\n",
    "embedding2 = tags_embedding.text_to_bow_vector(word)\n",
    "print(embedding2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200,)\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "['<NULL>', '<ROOT>', 'the', 'apple', 'trees', 'grow', '<NULL>', '<ROOT>', 'DT', 'NN', 'NNS', 'VB']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, embedding_size, words_embedding, tags_embedding):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.word_embeddings = words_embedding\n",
    "        self.tag_embeddings = tags_embedding\n",
    "\n",
    "    def get_word_embedding(self, word):\n",
    "        return self.word_embeddings.text_to_bow_vector(word)\n",
    "\n",
    "    def get_tag_embedding(self, tag):\n",
    "        return self.tag_embeddings.text_to_bow_vector(tag)\n",
    "\n",
    "    def extract_features(self, stack, buffer):\n",
    "        features = []\n",
    "        stack_extended = []\n",
    "        buffer_extended = []\n",
    "        stack_pos_extended = []\n",
    "        buffer_pos_extended = []\n",
    "\n",
    "        while len(stack) < 3:\n",
    "            stack.insert(0,(\"<NULL>\", \"<NULL>\"))\n",
    "        while len(buffer) < 3:\n",
    "            buffer.append((\"<NULL>\", \"<NULL>\"))\n",
    "\n",
    "        for i in range(3):\n",
    "            stack_word, stack_tag = stack[i]\n",
    "            buffer_word, buffer_tag = buffer[i]\n",
    "\n",
    "            stack_extended.append(stack_word)\n",
    "            stack_pos_extended.append(stack_tag)\n",
    "            buffer_extended.append(buffer_word)\n",
    "            buffer_pos_extended.append(buffer_tag)\n",
    "\n",
    "        for word in stack_extended + buffer_extended:\n",
    "            features.extend(self.get_word_embedding(word))\n",
    "        for tag in stack_pos_extended + buffer_pos_extended:\n",
    "            features.extend(self.get_tag_embedding(tag))\n",
    "\n",
    "        return np.array(features), stack_extended + buffer_extended + stack_pos_extended + buffer_pos_extended\n",
    "\n",
    "fe = FeatureExtractor(DIMENSION, words_embedding, tags_embedding)\n",
    "stack = [(\"<ROOT>\", \"<ROOT>\"), (\"the\", \"DT\")]\n",
    "buffer = [(\"apple\", \"NN\"), (\"trees\", \"NNS\"), (\"grow\", \"VB\")]\n",
    "features, feature_words = fe.extract_features(stack, buffer)\n",
    "print(features.shape)  \n",
    "print(features)\n",
    "print(feature_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Implement the scoring function (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector size: torch.Size([1, 1200])\n",
      "Scores for each action: tensor([[0.0111, 0.0102, 0.0085, 0.0176, 0.0066, 0.0112, 0.0126, 0.0291, 0.0107,\n",
      "         0.0129, 0.0168, 0.0105, 0.0089, 0.0152, 0.0177, 0.0156, 0.0112, 0.0103,\n",
      "         0.0134, 0.0103, 0.0142, 0.0143, 0.0137, 0.0149, 0.0098, 0.0101, 0.0088,\n",
      "         0.0086, 0.0135, 0.0226, 0.0106, 0.0102, 0.0121, 0.0116, 0.0088, 0.0123,\n",
      "         0.0155, 0.0089, 0.0139, 0.0112, 0.0158, 0.0143, 0.0085, 0.0085, 0.0118,\n",
      "         0.0116, 0.0103, 0.0127, 0.0195, 0.0109, 0.0083, 0.0126, 0.0208, 0.0145,\n",
      "         0.0129, 0.0135, 0.0123, 0.0096, 0.0108, 0.0133, 0.0134, 0.0149, 0.0136,\n",
      "         0.0085, 0.0179, 0.0163, 0.0119, 0.0080, 0.0134, 0.0189, 0.0115, 0.0147,\n",
      "         0.0107, 0.0128, 0.0121, 0.0165, 0.0112, 0.0151]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Scores size: torch.Size([1, 78])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class ScoringFunction(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_actions):\n",
    "        super(ScoringFunction, self).__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)  \n",
    "        x = torch.tanh(x)  \n",
    "        x = self.output(x) \n",
    "        return F.softmax(x, dim=-1)  \n",
    "\n",
    "\n",
    "input_size = DIMENSION * NUM_FEATURES  \n",
    "hidden_size = 200 \n",
    "num_actions = len(action_vocab) \n",
    "\n",
    "mlp = ScoringFunction(input_size, hidden_size, num_actions)\n",
    "\n",
    "phi_c = torch.randn(1, input_size)  \n",
    "print(\"Feature vector size:\", phi_c.size())\n",
    "\n",
    "scores = mlp(phi_c)\n",
    "print(\"Scores for each action:\", scores)\n",
    "print(\"Scores size:\", scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Implement the Training Step (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Parser(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_actions):\n",
    "        super(Parser, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor(embedding_size, words_embedding, tags_embedding)\n",
    "        self.scoring_function = ScoringFunction(embedding_size * 12, hidden_size, num_actions)\n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "\n",
    "        scores_list = []\n",
    "        for instance in X:\n",
    "            cur_scores = self.scoring_function(instance)\n",
    "                \n",
    "            scores_list.append(cur_scores)\n",
    "            \n",
    "        scores = torch.stack(scores_list)\n",
    "        return scores\n",
    "\n",
    "    def compute_loss(self, predicted_scores, y):\n",
    "        loss = F.cross_entropy(predicted_scores, y)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def process(dep_trees: List[DependencyTree], word_vocab: dict, pos_vocab: dict, action_vocab, words_embedding, tags_embedding) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    tensor_data = []\n",
    "    \n",
    "    for tree in dep_trees:\n",
    "        instances = get_training_instances(tree)\n",
    "      \n",
    "        for state, action in instances:\n",
    "            if action==('done', None):\n",
    "                continue\n",
    "            feature_extractor = FeatureExtractor(DIMENSION, words_embedding, tags_embedding)\n",
    "            feature_vector, _ = state_to_feature_vector(state, feature_extractor)\n",
    "\n",
    "           \n",
    "            action_tensor = torch.tensor(action_vocab.index(action), dtype=torch.long)\n",
    "\n",
    "            tensor_data.append({'X': feature_vector, 'y': action_tensor})\n",
    "\n",
    "    return tensor_data\n",
    "\n",
    "def state_to_feature_vector(state, feature_extractor):\n",
    "    stack = state.stack_words\n",
    "    buffer = state.stack_words\n",
    "   \n",
    "    \n",
    "    features, feature_words = feature_extractor.extract_features(stack, buffer)\n",
    "\n",
    "    feature_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "    return feature_tensor, feature_words\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data process finish\n",
      "dev_data process finish\n"
     ]
    }
   ],
   "source": [
    "train_data = process(train_trees,words_set,tags_set,action_vocab,words_embedding, tags_embedding)\n",
    "print(\"train_data process finish\")\n",
    "dev_data = process(dev_trees,words_set,tags_set,action_vocab,words_embedding, tags_embedding)\n",
    "print(\"dev_data process finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, dev_loader, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() \n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            X_batch, y_batch = batch['X'], batch['y']\n",
    "            optimizer.zero_grad() \n",
    "            predicted_scores = model(X_batch)\n",
    "            loss = model.compute_loss(predicted_scores, y_batch)\n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()  \n",
    "        total_loss = 0.0\n",
    "        for batch in dev_loader:\n",
    "            X_batch, y_batch = batch['X'], batch['y']\n",
    "            predicted_scores = model(X_batch)\n",
    "            loss = model.compute_loss(predicted_scores, y_batch)\n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "        dev_loss = total_loss / len(dev_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Dev Loss: {dev_loss:.4f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Train Loss: 3.8537, Dev Loss: 3.8522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate=0.001\n",
    "batch_size=32\n",
    "num_epochs=1\n",
    "hidden_size = 200\n",
    "num_actions = len(action_vocab)\n",
    "model = Parser(DIMENSION, hidden_size, num_actions)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_data, batch_size, shuffle=False)\n",
    "train_model(model, train_loader, dev_loader, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"parser_model_2.pth\"\n",
    "# torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_path = \"parser_model_1.pth\"\n",
    "# model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Implement the Inference Step (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Dependency Tree: [{'deprel': 'compound:prt', 'head': (1, 0)}, {'deprel': 'compound:prt', 'head': (2, 0)}, {'deprel': 'cc', 'head': (3, 0)}, {'deprel': 'nummod', 'head': (4, 0)}, {'deprel': 'cc', 'head': (5, 0)}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_sentence(model, sentence):\n",
    "\n",
    "    stack = [('<ROOT>', '<ROOT>')]\n",
    "    buffer = list(reversed(sentence))\n",
    "\n",
    "    location=list(sentence)\n",
    "\n",
    "    location.insert(0,('<ROOT>', '<ROOT>'))\n",
    "    tree = [{} for _ in sentence]\n",
    "    while len(stack) > 1 or len(buffer) > 0:\n",
    "\n",
    "        feature_extractor = FeatureExtractor(DIMENSION, words_embedding, tags_embedding)\n",
    "        features, _ = feature_extractor.extract_features(copy.copy(stack)    , copy.copy(buffer) )\n",
    "        feature_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)  # add batch dimension\n",
    "        scores = model(feature_tensor)\n",
    "        legal_mask=get_legal_mask(stack, buffer,copy.copy(action_vocab))\n",
    "\n",
    "        transition_idx = (scores +legal_mask ).argmax()\n",
    "        transition = action_vocab[transition_idx]\n",
    "        if transition[0] == 'shift' and buffer:\n",
    "            stack.append(buffer.pop())\n",
    "            \n",
    "        elif transition[0]=='left_arc':\n",
    "            \n",
    "            s1=stack[-1]\n",
    "            s2 = stack[-2]\n",
    "           \n",
    "            tree[location.index(s2)-1 ] = {\n",
    "                'deprel': transition[1],\n",
    "                'head': (location.index(s2), location.index(s1))\n",
    "            }\n",
    "            stack.pop(-2)\n",
    "        elif transition[0]=='right_arc':\n",
    "            s1=stack[-1]\n",
    "            s2 = stack[-2] \n",
    "           \n",
    "            tree[location.index(s1) -1] = {\n",
    "                'deprel': transition[1],\n",
    "                'head': (location.index(s1), location.index(s2))\n",
    "            }\n",
    "            stack.pop()\n",
    "        if len(stack) == 1 and not buffer:\n",
    "            break\n",
    "\n",
    "    return tree\n",
    "\n",
    "def get_legal_mask(stack, buffer,action_vocab):\n",
    "    mask = torch.full_like(scores, float('-inf'))\n",
    "    for i in range(len(action_vocab)):\n",
    "        action=action_vocab[i]\n",
    "        if len(stack)<2:\n",
    "            if action==('shift', None) :\n",
    "                mask[0][i] = 0\n",
    "            \n",
    "        else:\n",
    "            if not action==('shift', None) :\n",
    "                mask[0][i] = 0\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "# 1\tMs.\t_\t_\tNNP\t_\t2\tcompound\t_\t_\n",
    "# 2\tHaag\t_\t_\tNNP\t_\t3\tnsubj\t_\t_\n",
    "# 3\tplays\t_\t_\tVBZ\t_\t0\troot\t_\t_\n",
    "# 4\tElianti\t_\t_\tNNP\t_\t3\tdobj\t_\t_\n",
    "# 5\t.\t_\t_\t.\t_\t3\tpunct\t_\t_\n",
    "\n",
    "# Example sentence\n",
    "sentence = [('Ms.','NNP'), ('Haag','NNP'), ('plays','VBZ'), ('Elianti','NNP'),  ('.','.')]\n",
    "\n",
    "\n",
    "parsed_tree = parse_sentence(model,sentence)\n",
    "print(\"Parsed Dependency Tree:\", parsed_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('But', 'CC'),\n",
      " ('while', 'IN'),\n",
      " ('the', 'DT'),\n",
      " ('New', 'NNP'),\n",
      " ('York', 'NNP'),\n",
      " ('Stock', 'NNP'),\n",
      " ('Exchange', 'NNP'),\n",
      " ('did', 'VBD'),\n",
      " (\"n't\", 'RB'),\n",
      " ('fall', 'VB'),\n",
      " ('apart', 'RB'),\n",
      " ('Friday', 'NNP'),\n",
      " ('as', 'IN'),\n",
      " ('the', 'DT'),\n",
      " ('Dow', 'NNP'),\n",
      " ('Jones', 'NNP'),\n",
      " ('Industrial', 'NNP'),\n",
      " ('Average', 'NNP'),\n",
      " ('plunged', 'VBD'),\n",
      " ('190.58', 'CD'),\n",
      " ('points', 'NNS'),\n",
      " ('--', ':'),\n",
      " ('most', 'JJS'),\n",
      " ('of', 'IN'),\n",
      " ('it', 'PRP'),\n",
      " ('in', 'IN'),\n",
      " ('the', 'DT'),\n",
      " ('final', 'JJ'),\n",
      " ('hour', 'NN'),\n",
      " ('--', ':'),\n",
      " ('it', 'PRP'),\n",
      " ('barely', 'RB'),\n",
      " ('managed', 'VBD'),\n",
      " ('to', 'TO'),\n",
      " ('stay', 'VB'),\n",
      " ('this', 'DT'),\n",
      " ('side', 'NN'),\n",
      " ('of', 'IN'),\n",
      " ('chaos', 'NN'),\n",
      " ('.', '.')]\n",
      "----\n",
      "[{'deprel': 'cc', 'head': (1, 0)},\n",
      " {'deprel': 'cc', 'head': (2, 0)},\n",
      " {'deprel': 'cc', 'head': (3, 3)},\n",
      " {'deprel': 'cc', 'head': (4, 3)},\n",
      " {'deprel': 'cc', 'head': (5, 3)},\n",
      " {'deprel': 'cc', 'head': (6, 3)},\n",
      " {'deprel': 'cc', 'head': (7, 3)},\n",
      " {'deprel': 'cc', 'head': (8, 3)},\n",
      " {'deprel': 'cc', 'head': (9, 3)},\n",
      " {'deprel': 'cc', 'head': (10, 3)},\n",
      " {'deprel': 'cc', 'head': (11, 3)},\n",
      " {'deprel': 'cc', 'head': (12, 3)},\n",
      " {'deprel': 'cc', 'head': (13, 3)},\n",
      " {},\n",
      " {'deprel': 'cc', 'head': (15, 3)},\n",
      " {'deprel': 'cc', 'head': (16, 3)},\n",
      " {'deprel': 'cc', 'head': (17, 3)},\n",
      " {'deprel': 'cc', 'head': (18, 3)},\n",
      " {'deprel': 'cc', 'head': (19, 3)},\n",
      " {'deprel': 'cc', 'head': (20, 3)},\n",
      " {'deprel': 'cc', 'head': (21, 3)},\n",
      " {'deprel': 'cc', 'head': (22, 3)},\n",
      " {'deprel': 'cc', 'head': (23, 3)},\n",
      " {'deprel': 'cc', 'head': (24, 3)},\n",
      " {'deprel': 'cc', 'head': (25, 3)},\n",
      " {'deprel': 'cc', 'head': (26, 3)},\n",
      " {},\n",
      " {'deprel': 'cc', 'head': (28, 3)},\n",
      " {'deprel': 'cc', 'head': (29, 3)},\n",
      " {},\n",
      " {},\n",
      " {'deprel': 'cc', 'head': (32, 3)},\n",
      " {'deprel': 'cc', 'head': (33, 3)},\n",
      " {'deprel': 'cc', 'head': (34, 3)},\n",
      " {'deprel': 'cc', 'head': (35, 3)},\n",
      " {'deprel': 'cc', 'head': (36, 3)},\n",
      " {'deprel': 'cc', 'head': (37, 3)},\n",
      " {},\n",
      " {'deprel': 'cc', 'head': (39, 3)},\n",
      " {'deprel': 'cc', 'head': (40, 3)}]\n"
     ]
    }
   ],
   "source": [
    "predicted_trees = []\n",
    "test_sentences=[]\n",
    "\n",
    "for tree in test_trees:\n",
    "    sentence=[]\n",
    "    for i in range(1,len(tree.words())):\n",
    "        sentence.append((tree.words()[i],tree.pos()[i]))\n",
    "\n",
    "    test_sentences.append(sentence)\n",
    "\n",
    "for processed_instance in test_sentences:\n",
    "    parsed_tree = parse_sentence(model,processed_instance)\n",
    "    predicted_trees.append(parsed_tree)\n",
    "\n",
    "pprint(test_sentences[1])\n",
    "print(\"----\")\n",
    "pprint(predicted_trees[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Evaluation (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'deprel': 'discourse', 'head': (1, 7)},\n",
      " {'deprel': 'punct', 'head': (2, 7)},\n",
      " {'deprel': 'nsubj', 'head': (3, 7)},\n",
      " {'deprel': 'cop', 'head': (4, 7)},\n",
      " {'deprel': 'neg', 'head': (5, 7)},\n",
      " {'deprel': 'compound', 'head': (6, 7)},\n",
      " {'deprel': 'root', 'head': (7, 0)},\n",
      " {'deprel': 'punct', 'head': (8, 7)}]\n"
     ]
    }
   ],
   "source": [
    "test_trees_modified=[]\n",
    "count=0\n",
    "for tree in test_trees:\n",
    "\n",
    "    tempt=[]\n",
    "    for i in tree.deprels:\n",
    "        tempt.append(({'head':(i,tree.deprels[i].head),'deprel':tree.deprels[i].deprel}))\n",
    "       \n",
    "    test_trees_modified.append(tempt)\n",
    "       \n",
    "pprint(test_trees_modified[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS (Unlabeled Attachment Score): 0.0326\n",
      "LAS (Labeled Attachment Score): 0.0039\n"
     ]
    }
   ],
   "source": [
    "def calculate_uas_las(gold_tree, predicted_tree):\n",
    "    correct_heads = 0\n",
    "    correct_labels = 0\n",
    "    total = 0\n",
    "    \n",
    "    for gold_token, predicted_token in zip(gold_tree, predicted_tree):\n",
    "        if not predicted_token:\n",
    "            continue\n",
    "        \n",
    "        if gold_token['head'] == predicted_token['head']:\n",
    "            correct_heads += 1\n",
    "            if gold_token['deprel'] == predicted_token['deprel']:\n",
    "                correct_labels += 1\n",
    "        total += 1\n",
    "    \n",
    "    return correct_heads, correct_labels, total\n",
    "\n",
    "total_correct_heads = 0\n",
    "total_correct_labels = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for gold_tree, predicted_tree in zip(test_trees_modified, predicted_trees):\n",
    "\n",
    "    correct_heads, correct_labels, tokens = calculate_uas_las(gold_tree, predicted_tree)\n",
    "    total_correct_heads += correct_heads\n",
    "    total_correct_labels += correct_labels\n",
    "    total_tokens += tokens\n",
    "\n",
    "UAS_score = total_correct_heads / total_tokens\n",
    "LAS_score = total_correct_labels / total_tokens\n",
    "\n",
    "print(f\"UAS (Unlabeled Attachment Score): {UAS_score:.4f}\")\n",
    "print(f\"LAS (Labeled Attachment Score): {LAS_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

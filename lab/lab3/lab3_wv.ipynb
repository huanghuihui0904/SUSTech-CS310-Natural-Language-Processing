{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS310 Natural Language Processing\n",
    "# Lab 3: Word Vectors\n",
    "\n",
    "Install `scikit-learn`, `matplotlib`, and `gensim` first\n",
    "\n",
    "```bash\n",
    "pip install -U scikit-learn matplotlib gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Latent Semantic Analysis (LSA) - A word-document based approach\n",
    "\n",
    "We are working on the first ten chapters of 《论语》 (The Analects of Confucius) in this lab. The text is in Chinese (with punctuations).\n",
    "\n",
    "First, load the corpus data from the folder `lunyu` into a list of documents. We read all lines from the ten TXT files and combine them into a list of strings, `documents`. Each line of text is considered as a *document*.\n",
    "\n",
    "The file names are in the format of `text_ch1.txt`, `text_ch2.txt`, ..., `text_ch10.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n",
      "['子曰： 学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？', '有子曰： 其为人也孝弟，而好犯上者，鲜矣；不好犯上而好作乱者，未之有也。君子务本，本立而道生。孝弟也者，其为仁之本与！', '子曰： 巧言令色，鲜矣仁！']\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for i in range(1, 11):\n",
    "    with open(f\"lunyu/text_ch{i}.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            documents.append(line.strip())\n",
    "\n",
    "# Test result\n",
    "print(len(documents))\n",
    "print(documents[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean the data**\n",
    "\n",
    "Replace spaces `' '` with empty string `''`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['子曰：学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？',\n",
       " '有子曰：其为人也孝弟，而好犯上者，鲜矣；不好犯上而好作乱者，未之有也。君子务本，本立而道生。孝弟也者，其为仁之本与！',\n",
       " '子曰：巧言令色，鲜矣仁！']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START TODO ###\n",
    "documents = [document.replace(' ', '') for document in documents]\n",
    "### END TODO ###\n",
    "documents[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all documents into a single string `words`, whose length is the number of tokens in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START TODO ###\n",
    "words: str = ''.join(documents)\n",
    "### END TODO ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8664\n"
     ]
    }
   ],
   "source": [
    "# Test result\n",
    "print(len(words))\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# 8664"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build vocabulary**\n",
    "\n",
    "Use `collections.Counter` to build a vocabulary of characters (字) from the cleaned data. Yes, we do not use word segmentation here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "### START TODO ###\n",
    "vocab = Counter(words)\n",
    "### END TODO ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('，', 738), ('。', 405), ('子', 402), ('：', 339), ('曰', 318), ('不', 272), ('也', 254), ('之', 251), ('而', 155), ('？', 126)]\n"
     ]
    }
   ],
   "source": [
    "# Test result\n",
    "print(vocab.most_common(10))\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# [('，', 738), ('。', 405), ('子', 402), ('：', 339), ('曰', 318), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build `word2index`**\n",
    "\n",
    "Use the vocabulary to build a `word2index` dictionary, which maps each character to an integer index.\n",
    "\n",
    "The order of indices should be the descending order of character frequency, i.e., the most frequent character has index 0, the second most frequent character has index 1, and so on.\n",
    "\n",
    "For example: \n",
    "```python\n",
    ">>> word2index['，']\n",
    ">>> 0\n",
    ">>> word2index['。']\n",
    ">>> 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START TODO ###\n",
    "word2index = {char: idx for idx, (char, _) in enumerate(vocab.most_common())}\n",
    "### END TODO ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('，', 0), ('。', 1), ('子', 2), ('：', 3), ('曰', 4), ('不', 5), ('也', 6), ('之', 7), ('而', 8), ('？', 9)]\n"
     ]
    }
   ],
   "source": [
    "# Test result\n",
    "print(list(itertools.islice(word2index.items(), 10)))\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# [('，', 0), ('。', 1), ('子', 2), ('：', 3), ('曰', 4), ('不', 5), ('也', 6), ('之', 7), ('而', 8), ('？', 9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize word-document matrix**\n",
    "\n",
    "Use `numpy.zeros()` to initialize a word-document matrix `A` with shape `(vocab-size, num-documents)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(986, 267)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "### START TODO ###\n",
    "vocab_size = len(vocab) \n",
    "num_documents = len(documents) \n",
    "A = np.zeros((vocab_size, num_documents))\n",
    "### END TODO ###\n",
    "\n",
    "# Test result\n",
    "print(A.shape)\n",
    "print(A)\n",
    "# You should see a matrix of all zeros with the shape of (vocab_size, num_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill in the entries of word-document matrix**\n",
    "\n",
    "`A[i, j]` is the frequency of word `i` in document `j`. For simplicity, you can go through each document and count the frequency of each word that has appeared in the document.\n",
    "\n",
    "You may need the `word2index` dictionary to convert a word to its index, so that the correct row index `i` can be found. The column index `j` is the document index in the list `documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert A is all zeros; if not, run the previous cell to reset A\n",
    "assert np.all(A == 0)\n",
    "            \n",
    "### START TODO ###\n",
    "for i, d in enumerate(documents): \n",
    "    for character in d:  \n",
    "        if character in word2index: \n",
    "            w = word2index[character]  \n",
    "            A[w, i] += 1  \n",
    "### END TODO ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 5. 1. 0. 3.]\n",
      " [0. 2. 0. 0. 1.]\n",
      " [2. 2. 1. 1. 1.]\n",
      " [1. 1. 1. 2. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Test result\n",
    "print(A[:5, :5])\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# [[3. 5. 1. 0. 3.]\n",
    "#  [0. 2. 0. 0. 1.]\n",
    "#  [2. 2. 1. 1. 1.]\n",
    "#  [1. 1. 1. 2. 1.]\n",
    "#  [1. 1. 1. 1. 1.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute probabilities from the matrix**\n",
    "\n",
    "What is the total number of occurrences of the character \"学\" in the corpus?\n",
    "\n",
    "*Hint*: Use `numpy.sun()` on the correct subset of the matrix `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n"
     ]
    }
   ],
   "source": [
    "### START TODO ###\n",
    "index_of_char = word2index.get(\"学\") \n",
    "count = np.sum(A[index_of_char, :]) if index_of_char is not None else 0  \n",
    "### END TODO ###\n",
    "\n",
    "\n",
    "# Test result\n",
    "print(count)\n",
    "# You are expected to see the following output:\n",
    "# 25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the expected count of the character \"学\" in the first document -- \"子曰：学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？\" (`documents[0]`)?\n",
    "\n",
    "*Hint*: First, compute the global unigram probability of `学`, then multiply it by the number of words in the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11253462603878117\n"
     ]
    }
   ],
   "source": [
    "### START TODO ###\n",
    "index_of_学 = word2index[\"学\"]\n",
    "total_occurrences_of_学 = np.sum(A[index_of_学, :])\n",
    "total_number_of_characters = np.sum(A)\n",
    "global_unigram_probability_of_学 = total_occurrences_of_学 / total_number_of_characters\n",
    "\n",
    "prob = global_unigram_probability_of_学\n",
    "\n",
    "num_chars_in_first_doc=len(documents[0])\n",
    "expected_count = prob * num_chars_in_first_doc\n",
    "### END TODO ###\n",
    "\n",
    "\n",
    "# Test result\n",
    "print(expected_count)\n",
    "# You are expected to see the following output:\n",
    "# 0.11253462603878117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the *pointwise mutual information* (PMI) of the character \"学\", or \"surprise\" in `document[0]`? What about in document[1]?\n",
    "\n",
    "Complete the implementation of the function `PMI(word, doc, A, word2index, n_words, documents)`\n",
    "\n",
    "*Hint*: \n",
    "- Use the formula for PMI: $\\log(\\frac{\\text{observed count}}{\\text{expected count}})$\n",
    "- Handle the case when the observed count is 0, where the PMI is undefined. In this case, return 0.\n",
    "- The $[]_+$ operation means that we want to keep the PMI value positive. You can use `max()` to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1844943176829794\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "### START TODO ###\n",
    "def PMI(word: str, doc: int, \n",
    "    A: np.ndarray, word2index: dict, \n",
    "    n_words: int, documents: List[str]) -> float:\n",
    "    \"\"\" Compute the pointwise mutual information of a word in a document.\n",
    "    Args:\n",
    "        word: The word to compute PMI for.\n",
    "        doc: The document index.\n",
    "        A: The word-document count matrix.\n",
    "        word2index: The word to index mapping.\n",
    "        n_words: The number of words in the corpus.\n",
    "        documents: The list of documents.\n",
    "    \"\"\"\n",
    "    \"\"\" Compute the pointwise mutual information of a word in a document.\"\"\"\n",
    "    if word not in word2index:\n",
    "        return 0  \n",
    "    \n",
    "    observed_count = A[word2index[word], doc]\n",
    "    \n",
    "    if observed_count == 0:\n",
    "        return 0  \n",
    "    \n",
    "    word_total_count = np.sum(A[word2index[word], :])\n",
    "    doc_length = len(documents[doc])\n",
    "    expected_count = (word_total_count * doc_length) / n_words\n",
    "\n",
    "    pmi = max(0, np.log(observed_count / expected_count))\n",
    "    return pmi\n",
    "### END TODO ###\n",
    "\n",
    "# Test result\n",
    "print(PMI('学', 0, A, word2index, len(words), documents))\n",
    "print(PMI('学', 1, A, word2index, len(words), documents))\n",
    "# You are expected to see the following output:\n",
    "# 2.1844943176829794\n",
    "# 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement LSA using Truncated SVD**\n",
    "\n",
    "Use [sklearn.decomposition.TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html). (Check the documentation for more details)\n",
    "\n",
    "\n",
    "Set the number of components to 2, and fit the model with the word-document matrix `A`.\n",
    "\n",
    "Save the dimension reduced matrix to `M`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START TODO ###\n",
    "n_components = 2  \n",
    "svd = TruncatedSVD(n_components=n_components) \n",
    "M = svd.fit_transform(A) \n",
    "### END TODO ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(986, 2)\n",
      "[1.41745292 0.49414623]\n"
     ]
    }
   ],
   "source": [
    "# Test result\n",
    "print(M.shape)\n",
    "print(M[word2index['学'],:])\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# (986, 2)\n",
    "# [1.41745292 0.4941003 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some words in the 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = ['学', '习', '曰', '子', '人', '仁']\n",
    "words_pinyin = ['xue', 'xi', 'yue', 'zi', 'ren1', 'ren2']\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i, word in enumerate(words):\n",
    "    plt.scatter(M[word2index[word], 0], M[word2index[word], 1])\n",
    "    plt.text(M[word2index[word], 0], M[word2index[word], 1], words_pinyin[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Play with Pretrained Word Vectors\n",
    "\n",
    "Checkout the `gensim` library and its `downloader` API here: https://radimrehurek.com/gensim/downloader.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load Pretrained Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All embeddings, each lengh 200 (or 300 if using Google News)\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\") # file size 252.1MB, vocab size 400k, dim 200\n",
    "    # You can also try \"word2vec-google-news-300\", which is much larger 1600+MB\n",
    "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "    return wv_from_bin\n",
    "\n",
    "\n",
    "# Run this to get the full corpora information\n",
    "# import gensim.downloader as api\n",
    "# api.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 400000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Run Cell to Load Word Vectors\n",
    "# Note: This will take a couple minutes\n",
    "# -----------------------------------\n",
    "wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with word analogy task using the `most_similar` method of the `KeyedVectors` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('grandmother', 0.7608445286750793),\n",
      " ('granddaughter', 0.7200808525085449),\n",
      " ('daughter', 0.7168302536010742),\n",
      " ('mother', 0.7151536345481873),\n",
      " ('niece', 0.7005682587623596),\n",
      " ('father', 0.6659888029098511),\n",
      " ('aunt', 0.6623408794403076),\n",
      " ('grandson', 0.6618767380714417),\n",
      " ('grandparents', 0.6446609497070312),\n",
      " ('wife', 0.6445354223251343)]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# Run this cell to answer the analogy -- man : grandfather :: woman : x\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'grandfather'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the above example and try any other analogy tasks you like.\n",
    "\n",
    "For instance, China: Beijing = Japan: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('brother', 0.7270498871803284),\n",
      " ('father', 0.6531276702880859),\n",
      " ('cousin', 0.6404421329498291),\n",
      " ('son', 0.6370454430580139),\n",
      " ('uncle', 0.6317039728164673),\n",
      " ('sons', 0.6011109352111816),\n",
      " ('friend', 0.5927498936653137),\n",
      " ('nephew', 0.5820999145507812),\n",
      " ('mother', 0.5816723704338074),\n",
      " ('daughter', 0.561065137386322)]\n"
     ]
    }
   ],
   "source": [
    "### START TODO ###\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'sister'], negative=['woman']))\n",
    "### END TODO ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

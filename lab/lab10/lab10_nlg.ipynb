{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 10: Explore Natural Language Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Explore Pretrained GPT-2 Model\n",
    "\n",
    "In this task, you will explore the GPT-2 model using the `transformers` library.\n",
    "\n",
    "Just like in the previous lab, you will need to download the pretrained model and unzip it to `./gpt2zh`. Note that this is not the original version of GPT-2 provided by OpenAI (https://huggingface.co/openai-community/gpt2), but rather a fine-tuned version for Chinese text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 21128\n",
      "special token [SEP]: 102\n",
      "special token [CLS]: 101\n",
      "special token [PAD]: 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"./gpt2zh\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"./gpt2zh\")\n",
    "# Evaluation mode\n",
    "gpt2_model.eval()\n",
    "\n",
    "print('vocab size:', gpt2_tokenizer.vocab_size)\n",
    "print(f'special token {gpt2_tokenizer.sep_token}:', gpt2_tokenizer.sep_token_id)\n",
    "print(f'special token {gpt2_tokenizer.cls_token}:', gpt2_tokenizer.cls_token_id)\n",
    "print(f'special token {gpt2_tokenizer.pad_token}:', gpt2_tokenizer.pad_token_id)\n",
    "\n",
    "# Use [SEP] as end-of-sentence token\n",
    "gpt2_model.config.eos_token_id = gpt2_tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer can return the token IDs and the attention mask that indicates which tokens are padding tokens (`1` for real tokens, `0` for padding tokens).\n",
    "\n",
    "Since we only have one sentence in the \"batch\", there is no padding used, and thus no `0` in the attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids: tensor([[ 101, 2110, 5445, 3198,  739,  722, 8024,  679,  771, 6432,  725, 8013,\n",
      "          102]])\n",
      "input attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "input tokens: ['[CLS]', '学', '而', '时', '习', '之', '，', '不', '亦', '说', '乎', '！', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "input_text = '学而时习之，不亦说乎！'\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "print('input ids:', input_encoded['input_ids'])\n",
    "print('input attention mask:', input_encoded['attention_mask'])\n",
    "\n",
    "# Map token ids back to tokens\n",
    "print('input tokens:', gpt2_tokenizer.convert_ids_to_tokens(input_encoded['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to directly use the `generate` method to generate some sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "子 曰 ： 人 民 之 所 以 为 国 ， 民 之 所 以 为 民 ， 民 之\n",
      "子 曰 ： 人 皆 谤 谤 谤 谤 谤 谤 谤 谤 谤 谤 谤 谤 谤 谤 谤\n",
      "子 曰 ： 人 之 所 以 为 天 ， 人 之 所 以 为 地 ， 地 之 所\n",
      "子 曰 ： 人 民 不 应 该 向 他 们 求 助 ， 因 为 人 民 是 天\n",
      "子 曰 ： 人 道 之 善 ， 而 不 可 为 之 。 （ 原 文 ）\n"
     ]
    }
   ],
   "source": [
    "input_text = \"子曰：人\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "n_outputs = 5\n",
    "\n",
    "output = gpt2_model.generate(**input_encoded, \n",
    "                                 max_length=20, \n",
    "                                 num_return_sequences=n_outputs,\n",
    "                                 do_sample=True, \n",
    "                                 top_k=50, \n",
    "                                 top_p=0.95, \n",
    "                                 temperature=0.7,\n",
    "                                 pad_token_id=0,\n",
    "                                 )\n",
    "# print(type(output))\n",
    "# print(output.shape)\n",
    "\n",
    "for i in range(n_outputs):\n",
    "    output_text = gpt2_tokenizer.decode(output[i], skip_special_tokens=True)\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the generation is far from perfect. It still has good chances to produce a lot of repetitions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Implement Top-k Sampling Algorithms Manually\n",
    "\n",
    "Let's first try greedy search, i.e., top-1 sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: 4\n",
      "torch.Size([1, 4, 21128])\n",
      "预\n"
     ]
    }
   ],
   "source": [
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "print('input size:', input_encoded.input_ids.shape[1])\n",
    "\n",
    "output = gpt2_model(input_encoded.input_ids, \n",
    "                    attention_mask=input_encoded.attention_mask)\n",
    "logits = output.logits\n",
    "print(logits.shape)\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the probability distribution predicted at the last token's position\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get the most likely token id from this distribution\n",
    "most_likely_token_id = torch.argmax(last_token_logits).item()\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Convert the token id to a token\n",
    "most_likely_token = gpt2_tokenizer.convert_ids_to_tokens(most_likely_token_id)\n",
    "print(most_likely_token)\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# input size: 4\n",
    "# torch.Size([1, 4, 21128])\n",
    "# 预"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done with the above code, you can now implement the full generation loop: at each iteration, you select the most likely token and append it to the end input, and then feed the new input to the model for predicting the next token. \n",
    "\n",
    "The loop continues until `max_gen_len` is reached, or a `\"[SEP]\"` token is generated.\n",
    "\n",
    "**Note**: \n",
    "- Use `torch.cat` to append elements to input IDs\n",
    "- The `attn_mask` also needs be updated at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气预报：今天白天，我市阴天有小雨，气温：小雨转多云，气温：小雨转多云，气温：小雨转多云，气温：小雨转多"
     ]
    }
   ],
   "source": [
    "max_gen_len = 50\n",
    "\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "count = 0\n",
    "while count < max_gen_len:\n",
    "    output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "    logits = output.logits\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    sampled_token_id = torch.argmax(last_token_logits).item()\n",
    "    if sampled_token_id == gpt2_tokenizer.sep_token_id:\n",
    "        break\n",
    "    input_ids = torch.cat([input_ids, torch.tensor([[sampled_token_id]], dtype=torch.long)], dim=1) \n",
    "    attn_mask = torch.cat([attn_mask, torch.tensor([[1]], dtype=torch.long)], dim=1)\n",
    "   ### END YOUR CODE ###\n",
    "\n",
    "    count += 1\n",
    "\n",
    "\n",
    "# Test\n",
    "special_token_ids = set([gpt2_tokenizer.sep_token_id, \n",
    "                         gpt2_tokenizer.cls_token_id, \n",
    "                         gpt2_tokenizer.pad_token_id,\n",
    "                         100]) # 100 for [UNK]\n",
    "\n",
    "# Decode the generated tokens ids\n",
    "for i in range(input_ids.shape[1]):\n",
    "    tok_id = input_ids[0, i].item()\n",
    "    # Skip the special tokens\n",
    "    if tok_id not in special_token_ids:\n",
    "        print(gpt2_tokenizer.convert_ids_to_tokens(input_ids[0, i].item()), end='')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# 今天天气预报：今天白天，我市阴天有小雨，气温：小雨转多云，气温：小雨转多云，气温：小雨转多云，气温：小雨转多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, greedy search results in very repetitive text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement a `top-k` sampling algorithm.\n",
    "\n",
    "The idea is to **uniformly** sample from top-k most likely next tokens. PyTorch tensor provides a `topk` method to get the top-k values and indices. \n",
    "\n",
    "In the following example, you can check the top 5 most likely words following the sentence \"今天天气\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "### START YOUR CODE ###\n",
    "last_token_logits = logits[0, -1, :]\n",
    "top_k_logits, top_k_indices = torch.topk(last_token_logits, k)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print(top_k_logits)\n",
    "print(top_k_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = top_k_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's integrate the top-k sampling algorithm into the generation process. The uniform sampling can be implemented using `random.choices` among the top-k indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topk(input_text, k=5, max_gen_len=50):\n",
    "    input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    input_ids = input_encoded.input_ids\n",
    "    attn_mask = input_encoded.attention_mask\n",
    "\n",
    "    count = 0\n",
    "    while count < max_gen_len:\n",
    "        output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "        logits = output.logits\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "        last_token_logits = logits[0, -1, :]\n",
    "        top_k_logits, top_k_indices = torch.topk(last_token_logits, k)\n",
    "        sampled_token_id = random.choices(top_k_indices, k=1)[0].item()\n",
    "        if sampled_token_id == gpt2_tokenizer.sep_token_id:\n",
    "            break\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[sampled_token_id]], dtype=torch.long)], dim=1)\n",
    "        attn_mask = torch.cat([attn_mask, torch.tensor([[1]], dtype=torch.long)], dim=1)\n",
    "        \n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "    special_token_ids = set([gpt2_tokenizer.sep_token_id, \n",
    "                         gpt2_tokenizer.cls_token_id, \n",
    "                         gpt2_tokenizer.pad_token_id,\n",
    "                         100]) # 100 for [UNK]\n",
    "    \n",
    "    generated_text = ''\n",
    "    for i in range(input_ids.shape[1]):\n",
    "        tok_id = input_ids[0, i].item()\n",
    "        if tok_id not in special_token_ids:\n",
    "            generated_text += gpt2_tokenizer.convert_ids_to_tokens(tok_id)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气有不佳风的状的关云山不用谢一只会跑过日式风味，比不鸟好喝到哭有逼感！▼「牛羊河的早晚黑米粥店-这些\n",
      "子曰：人应在于对恶事事成有评分（比法之成见得不止五处是错处.我对过往）及大概想的只能去猜得了如不思反感以免在\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "input_text = \"今天天气\"\n",
    "print(generate_topk(input_text, k=50))\n",
    "\n",
    "input_text = \"子曰：人\"\n",
    "print(generate_topk(input_text, k=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that although the above uniform top-k sampling solves repetition issue, it will however produce *extremely incoherent* text. We can remedy this by using a proportional sampling instead of uniform sampling.\n",
    "\n",
    "There are plenty of different ways to implement proportionaly sampling. You can either:\n",
    "- Create list of cumulative relative probabilities of the top k tokens. For instance, if the relative probabilities of $k=5$ tokens are $0.1$, $0.2$, $0.5$, $0.1$, and $0.1$, then you cumulative probability list is `cum_prob = [0.1, 0.3, 0.8, 0.9, 1.0]`. Then you draw a random number $r$ from the unifrom distribution $[0,1]$ by `random.random()`, and you decide which token is sampled by telling which bin of `cum_prob` that $r$ falls into.\n",
    "- Or, you use the `torch.multinomial()` function to accomplish similar sampling. *Note* the input weight provided to `torch.multinomial` should be teh relative probabilities of the top $k$ tokens, which can be obtained from applying softmax to the logits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  \n",
    "\n",
    "def generate_topk_prop(input_text, k=50, max_gen_len=50):\n",
    "    input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    input_ids = input_encoded.input_ids\n",
    "    attn_mask = input_encoded.attention_mask\n",
    "\n",
    "    count = 0\n",
    "    while count < max_gen_len:\n",
    "        output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "        logits = output.logits\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "        top_k_logits, top_k_indices = torch.topk(logits[0, -1, :], k)\n",
    "        top_k_probabilities = F.softmax(top_k_logits, dim=0)\n",
    "        cum_probabilities = torch.cumsum(top_k_probabilities, dim=0)\n",
    "        random_value = random.random()\n",
    "        sampled_token_idx = (cum_probabilities >= random_value).nonzero()[0].item()\n",
    "        sampled_token_id = top_k_indices[sampled_token_idx].item()\n",
    "        if sampled_token_id == gpt2_tokenizer.sep_token_id:\n",
    "            break\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[sampled_token_id]], dtype=torch.long)], dim=1)\n",
    "        attn_mask = torch.cat([attn_mask, torch.tensor([[1]], dtype=torch.long)], dim=1)\n",
    "       \n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "    special_token_ids = set([gpt2_tokenizer.sep_token_id, \n",
    "                         gpt2_tokenizer.cls_token_id, \n",
    "                         gpt2_tokenizer.pad_token_id,\n",
    "                         100]) # 100 for [UNK]\n",
    "    \n",
    "    generated_text = ''\n",
    "    for i in range(input_ids.shape[1]):\n",
    "        tok_id = input_ids[0, i].item()\n",
    "        if tok_id not in special_token_ids:\n",
    "            generated_text += gpt2_tokenizer.convert_ids_to_tokens(tok_id)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气预报是这样的：小风吹着，空气湿度加大，但是我们的心里却有一种别样的感受：它能让天地交错，风一吹，就会\n",
      "子曰：人生如此，顺理成章。无论如何，你的生命是美丽的，而不是单一的、模糊的其实，这句话的意思在于，它是\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "input_text = \"今天天气\"\n",
    "print(generate_topk_prop(input_text, k=50))\n",
    "\n",
    "input_text = \"子曰：人\"\n",
    "print(generate_topk_prop(input_text, k=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the proportional sampling produces better text?\n",
    "\n",
    "Have fun sampling! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

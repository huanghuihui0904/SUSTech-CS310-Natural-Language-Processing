{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 9: Explore BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Explore Pretrained BERT Model\n",
    "\n",
    "In this task, you will explore the pretrained BERT model using the Hugging Face Transformers library. \n",
    "\n",
    "First, you will load a pretrained BERT model and the correponding tokenizer. If you use the default model string `'bert-base-uncased'`, it will automatically download the model.\n",
    "\n",
    "In our case, to avoid any network issue, you can follow these steps to load the model locally:\n",
    "- Download the `bert-base-uncased.zip` file from the course website and unzip it to the folder `bert-base-uncased` in the same directory as this notebook. \n",
    "- When you load the model, you simply specify the folder path `bert-base-uncased/` (which contains all model files) to the `from_pretrained()` function. \n",
    "- *Note* that don't exclude the last `/` in the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased/',output_hidden_states=True) # Make sure you download the model files first\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by counting the number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tensors:  199\n"
     ]
    }
   ],
   "source": [
    "n_tensors = 0\n",
    "for param in bert_model.parameters():\n",
    "    n_tensors += 1\n",
    "\n",
    "print(\"Number of tensors: \", n_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  109482240\n"
     ]
    }
   ],
   "source": [
    "n_params = 0\n",
    "for param in bert_model.parameters():\n",
    "    n_params += param.numel()\n",
    "\n",
    "print(\"Number of parameters: \", n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, if you are interested in how the parameters are organized, you can print the model's `_modules` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('embeddings', BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")), ('encoder', BertEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")), ('pooler', BertPooler(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      "))])\n"
     ]
    }
   ],
   "source": [
    "print(bert_model._modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, you can access the parameters at any layer of the model, by specifying the layer name and index. \n",
    "\n",
    "For example, if you want to check the the query matrix $W^Q$ in the self-attention layer of the first transformer block, you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True)\n"
     ]
    }
   ],
   "source": [
    "pprint(bert_model._modules['encoder']._modules['layer'][0]._modules['attention']._modules['self']._modules['query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the $W^Q$ matrix is implemented as a `nn.Linear` module.\n",
    "\n",
    "Also, the same inquiry can be simplified by using the `get_submodule()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "W_q = bert_model.get_submodule('encoder.layer.0.attention.self.query')\n",
    "print(W_q)\n",
    "print(W_q.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Get Contextual Embeddings from BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on and use the BERT model to get contextual embeddings for given texts.\n",
    "\n",
    "First, we prepare some sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I have a new CPU!',\n",
      " 'I have a new Intel CPU!',\n",
      " 'I have a new GPU!',\n",
      " 'I have a new NVIDIA GPU!']\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "        'I have a new CPU!\\n'\n",
    "        'I have a new Intel CPU!\\n'\n",
    "        'I have a new GPU!\\n'\n",
    "        'I have a new NVIDIA GPU!'\n",
    "    )\n",
    "\n",
    "sentences = text.split('\\n')\n",
    "pprint(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the `tokenize()` function of the previously initialized BERT tokenizer on each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'have', 'a', 'new', 'cpu', '!'],\n",
      " ['i', 'have', 'a', 'new', 'intel', 'cpu', '!'],\n",
      " ['i', 'have', 'a', 'new', 'gp', '##u', '!'],\n",
      " ['i', 'have', 'a', 'new', 'n', '##vid', '##ia', 'gp', '##u', '!']]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "tokens_in_string: List[str] = [bert_tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "pprint(tokens_in_string)\n",
    "# You should expect to see the following output:\n",
    "# [['i', 'have', 'a', 'new', 'cpu', '!'],\n",
    "#  ['i', 'have', 'a', 'new', 'intel', 'cpu', '!'],\n",
    "#  ['i', 'have', 'a', 'new', 'gp', '##u', '!'],\n",
    "#  ['i', 'have', 'a', 'new', 'n', '##vid', '##ia', 'gp', '##u', '!']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that \"CPU\" and \"Intel\" are recognized as whole words, but \"NVIDIA\" and \"GPU\" are not. Thus, they appear as subwords such as \"##u\" \"##vid\" in the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results are not integer token IDs yet, so now use the `batch_encode()` function, with argument `return_tensors='pt'`, to convert each sentence to integer token IDs. Here `'pt'` is for PyTorch tensors.\n",
    "\n",
    "**Note**:\n",
    "- Each token is represented as an integer in `torch.int64` data type.\n",
    "- By default, the tokenizer adds special tokens `[CLS]` and `[SEP]` to the beginning and end of each sentence, which correpond to the token ID `101` and `102`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "[tensor([[  101,  1045,  2031,  1037,  2047, 17368,   999,   102]]),\n",
      " tensor([[  101,  1045,  2031,  1037,  2047, 13420, 17368,   999,   102]]),\n",
      " tensor([[  101,  1045,  2031,  1037,  2047, 14246,  2226,   999,   102]]),\n",
      " tensor([[  101,  1045,  2031,  1037,  2047,  1050, 17258,  2401, 14246,  2226,\n",
      "           999,   102]])]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "token_ids_list: List[torch.Tensor] = [\n",
    "    bert_tokenizer.batch_encode_plus([sentence], return_tensors='pt')['input_ids'] \n",
    "    for sentence in sentences\n",
    "]\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print(token_ids_list[0].dtype)\n",
    "pprint(token_ids_list)\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# torch.int64\n",
    "# [tensor([[  101,  1045,  2031,  1037,  2047, 17368,   999,   102]]),\n",
    "#  tensor([[  101,  1045,  2031,  1037,  2047, 13420, 17368,   999,   102]]),\n",
    "#  tensor([[  101,  1045,  2031,  1037,  2047, 14246,  2226,   999,   102]]),\n",
    "#  tensor([[  101,  1045,  2031,  1037,  2047,  1050, 17258,  2401, 14246,  2226,\n",
    "#            999,   102]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now `\"CPU\"` is tokenized to `17368`, `\"Intel\"` to `13420`, while `\"GPU\"` to `[14246, 2226]`, and `\"NVIDIA\"` to `[1050, 17258,  2401]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `ids_to_tokens` dictionary to map integer token IDs back to token strings, and use `decode()` function to convert a list of token IDs back to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "[SEP]\n",
      "cpu\n",
      "[CLS] i have a new cpu! [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.ids_to_tokens[101])\n",
    "print(bert_tokenizer.ids_to_tokens[102])\n",
    "print(bert_tokenizer.ids_to_tokens[17368])\n",
    "print(bert_tokenizer.decode(token_ids_list[0].squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in last example above, we `squeeze` the token IDs first, becaseu the encoded IDs are of dimension $1\\times N$, where $N$ is sentence length, because PyTorch uses first dimension as batch size.\n",
    "\n",
    "It indicates that we can tokenize multiple sentences in one batch by using the `batch_encode_plus()` function, and specify the argument `padding=True` to pad all sentences to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2031,  1037,  2047, 17368,   999,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1045,  2031,  1037,  2047, 13420, 17368,   999,   102,     0,\n",
      "             0,     0],\n",
      "        [  101,  1045,  2031,  1037,  2047, 14246,  2226,   999,   102,     0,\n",
      "             0,     0],\n",
      "        [  101,  1045,  2031,  1037,  2047,  1050, 17258,  2401, 14246,  2226,\n",
      "           999,   102]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = bert_tokenizer.batch_encode_plus(sentences, return_tensors='pt', padding=True, return_attention_mask=False, return_token_type_ids=False)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the returned dictionary contains an item keyed by `'input_ids'`, which is exactly the token IDs we need. \n",
    "\n",
    "**Note**:\n",
    "- It is a tensor of shape $B\\times N$, where $B$ is the batch size (here, $B=4$) and $N$ is the maximum sentence length in the batch.\n",
    "- The default padding token is `0`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we deliberately set `return_attention_mask=False` to show simpler results. \n",
    "\n",
    "If you set it to `True`, then then returned dictionary will also contain an item keyed by `'attention_mask'`, which is a tensor of shape $B\\times N$ with `1` for real tokens and `0` for padding tokens. This information is useful for follow-up computations.\n",
    "\n",
    "Try if you can get the attention mask tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "encoded_sentences = bert_tokenizer.batch_encode_plus(\n",
    "    sentences, \n",
    "    return_tensors='pt',  # 'pt' specifies PyTorch tensors\n",
    "    padding=True,  # This ensures padding to the longest sentence\n",
    "    truncation=True,  # Truncates longer sequences\n",
    "    return_attention_mask=True  # Generate attention masks\n",
    ")\n",
    "\n",
    "# Extract the attention mask\n",
    "attn_mask = encoded_sentences['attention_mask']\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print(attn_mask)\n",
    "# You should expect to see the following output:\n",
    "# tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's obtain the contextual embeddings for the four target words `\"CPU\"`, `\"Intel\"`, `\"NVIDIA\"`, and `\"GPU\"` in our sentences.\n",
    "\n",
    "First, pass the token IDs in one batch to the BERT model to get the output object, which has a `last_hidden_state` attribute that contains the contextual embeddings.\n",
    "\n",
    "**Note**:\n",
    "- You can manually specify `input_ids` and `attention_mask` as the input arguments to the model.\n",
    "- Or you can directly pass the dictionary returned by `batch_encode_plus()` to the model, and use the `**` operator as most tutorials did:\n",
    "  - `outputs = model(**encoded_sentences)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "bert_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ### START YOUR CODE ###\n",
    "    outputs =bert_model(**encoded_sentences) \n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "print(outputs.last_hidden_state.shape)\n",
    "# You should expect to see the following output:\n",
    "# torch.Size([4, 12, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for ``\"CPU\"`` and ``\"Intel\"``, you can directly use the output vectors at the corresponding positions, because they are recognized as whole words.\n",
    "\n",
    "Compute the average vector of `\"CPU\"`s in the first two sentences, and compute its cosine similarity with the vector of `\"Intel\"`.\n",
    "\n",
    "*Hint*:\n",
    "- Use `F.cosine_similarity()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_cpu_intel: 0.7551645636558533\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "\n",
    "\n",
    "vec_cpu1 = outputs.last_hidden_state[0][5] \n",
    "vec_cpu2 = outputs.last_hidden_state[1][6] \n",
    "vec_cpu_avg = (vec_cpu1 + vec_cpu2) / 2\n",
    "\n",
    "\n",
    "vec_intel = outputs.last_hidden_state[1][5]  \n",
    "\n",
    "\n",
    "cos_cpu_intel = F.cosine_similarity(vec_cpu_avg.unsqueeze(0), vec_intel.unsqueeze(0))\n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('cos_cpu_intel:', cos_cpu_intel.item())\n",
    "# You should expect to see the following output:\n",
    "# cos_cpu_intel: 0.7551645636558533"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `\"NVIDIA\"` and `\"GPU\"`, it's a bit trickier, as you need to use the sum of subword vectors to get the vector of the whole word.\n",
    "\n",
    "In sentence 3, `\"GPU\"` is tokenized to `[14246, 2226]`, so you need to sum the vectors at these two positions.\n",
    "\n",
    "In sentence 4, `\"NVIDIA\"` is tokenized to `[1050, 17258,  2401]`, so you need to sum the vectors at these three positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_gpu_nv: 0.7273837327957153\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "vec_gpu1 = outputs.last_hidden_state[2][5]  \n",
    "vec_gpu2 = outputs.last_hidden_state[2][6]  \n",
    "vec_gpu3 = outputs.last_hidden_state[3][8]  \n",
    "vec_gpu4 = outputs.last_hidden_state[3][9] \n",
    "vec_gpu = (vec_gpu1 + vec_gpu2  +vec_gpu3+vec_gpu4)/4\n",
    "\n",
    "vec_nv1 = outputs.last_hidden_state[3][5]  \n",
    "vec_nv2 = outputs.last_hidden_state[3][6]  \n",
    "vec_nv3 = outputs.last_hidden_state[3][7] \n",
    "vec_nvidia = (vec_nv1 + vec_nv2 + vec_nv3 )/3 \n",
    "\n",
    "cos_gpu_nv = F.cosine_similarity(vec_gpu.unsqueeze(0), vec_nvidia.unsqueeze(0))\n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('cos_gpu_nv:', cos_gpu_nv.item())\n",
    "# You should expect to see the following output:\n",
    "# cos_gpu_nv: 0.7273837327957153\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if `\"NVIDIA\"` is closer to `\"GPU\"` than `\"CPU\"`, and vice versa for `\"Intel\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_cpu_nv: 0.5931224822998047\n",
      "cos_gpu_intel: 0.5778650045394897\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "cos_cpu_nv = F.cosine_similarity(vec_cpu_avg.unsqueeze(0), vec_nvidia.unsqueeze(0))\n",
    "cos_gpu_intel = F.cosine_similarity(vec_gpu.unsqueeze(0), vec_intel.unsqueeze(0))\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('cos_cpu_nv:', cos_cpu_nv.item())\n",
    "print('cos_gpu_intel:', cos_gpu_intel.item())\n",
    "# You should expect to see the following output:\n",
    "# cos_cpu_nv: 0.5931224226951599\n",
    "# cos_gpu_intel: 0.5778647661209106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's interesting, right?\n",
    "\n",
    "How about the distance between the two products `\"CPU\"` and `\"GPU\"`? or between the two companies `\"Intel\"` and `\"NVIDIA\"`? Check it out yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_cpu_gpu: 0.6914966106414795\n",
      "cos_intel_nv: 0.617974579334259\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "cos_cpu_gpu = F.cosine_similarity(vec_cpu_avg.unsqueeze(0), vec_gpu.unsqueeze(0))\n",
    "cos_intel_nv = F.cosine_similarity(vec_intel.unsqueeze(0), vec_nvidia.unsqueeze(0))\n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('cos_cpu_gpu:', cos_cpu_gpu.item())\n",
    "print('cos_intel_nv:', cos_intel_nv.item())\n",
    "# You should expect to see the following output:\n",
    "# cos_cpu_gpu: 0.6914964914321899\n",
    "# cos_intel_nv: 0.6179742813110352"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3. Access all hidden states\n",
    "\n",
    "Let's be more adventurous and access all hidden states returned by the BERT model.\n",
    "\n",
    "*Hint*: Simply set the argument `output_hidden_states=True` when calling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "13\n",
      "torch.Size([4, 12, 768])\n",
      "torch.Size([4, 12, 768])\n",
      "torch.Size([4, 12, 768])\n",
      "torch.Size([4, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "bert_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ### START YOUR CODE ###\n",
    "    outputs = bert_model(**encoded_sentences)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print(type(outputs.hidden_states))\n",
    "print(len(outputs.hidden_states))\n",
    "print(outputs.hidden_states[-1].shape)\n",
    "print(outputs.hidden_states[-2].shape)\n",
    "print(outputs.hidden_states[-3].shape)\n",
    "print(outputs.hidden_states[-4].shape)\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# <class 'tuple'>\n",
    "# 13\n",
    "# torch.Size([4, 12, 768])\n",
    "# torch.Size([4, 12, 768])\n",
    "# torch.Size([4, 12, 768])\n",
    "# torch.Size([4, 12, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average vector of the word `\"CPU\"` in the first sentence, using the hidden states of the last **four** layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos: 0.9002149701118469\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "cpu_pos_1 = 5\n",
    "hidden_state_layer_13 = outputs.hidden_states[-1] \n",
    "hidden_state_layer_12 = outputs.hidden_states[-2]  \n",
    "hidden_state_layer_11 = outputs.hidden_states[-3]  \n",
    "hidden_state_layer_10 = outputs.hidden_states[-4]  \n",
    "\n",
    "vec_cpu_13 = hidden_state_layer_13[0][cpu_pos_1]  \n",
    "vec_cpu_12 = hidden_state_layer_12[0][cpu_pos_1] \n",
    "vec_cpu_11 = hidden_state_layer_11[0][cpu_pos_1] \n",
    "vec_cpu_10 = hidden_state_layer_10[0][cpu_pos_1]  \n",
    "\n",
    "vec_cpu_avg_last4 = (vec_cpu_10 + vec_cpu_11 + vec_cpu_12 + vec_cpu_13) / 4\n",
    "\n",
    "cos = F.cosine_similarity(vec_cpu_avg_last4, vec_cpu_avg, dim=0) \n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "cos = F.cosine_similarity(vec_cpu_avg_last4, vec_cpu_avg, dim=0)\n",
    "print('cos:', cos.item())\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# cos: 0.9002149701118469"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

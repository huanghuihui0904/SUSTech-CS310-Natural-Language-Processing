{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 5 (part 2): Data preparation for Named Entity Recognition (NER)\n",
    "\n",
    "The dataset is CoNLL2003 English named entity recognition (NER). The dataset is a collection of news articles from Reuters. \n",
    "\n",
    "The dataset is annotated with four types of named entities: persons, locations, organizations, and miscellaneous entities that do not belong to the previous three types. The dataset is divided into three parts: **training**, **development**, and **testing**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/train.txt'\n",
    "DEV_PATH = 'data/dev.txt'\n",
    "TEST_PATH = 'data/test.txt'\n",
    "EMBEDDINGS_PATH = 'data/glove.6B.100d.txt' \n",
    "# Download from https://nlp.stanford.edu/data/glove.6B.zip\n",
    "# It includes dimension 50, 100, 200, and 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is in the IOB format. \n",
    "The IOB format is a simple text chunking format that divides the text into chunks and assigns a label to each chunk. The label is a combination of two parts: the type of the named entity and the position of the word in the named entity. The type of the named entity is one of the four types mentioned above. The position of the word in the named entity is one of three positions: B (beginning), I (inside), and O (outside). For example, the word \"New\" in the named entity \"New York\" is labeled as \"B-LOC\" and the word \"York\" is labeled as \"I-LOC\". The word \"I\" in the sentence \"I live in New York\" is labeled as \"O\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ner_data(path_to_file):\n",
    "    words = []\n",
    "    tags = []\n",
    "    with open(path_to_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            splitted = line.split()\n",
    "            if len(splitted) == 0:\n",
    "                continue\n",
    "            word = splitted[0]\n",
    "            if word == '-DOCSTART-':\n",
    "                continue\n",
    "            entity = splitted[-1]\n",
    "            words.append(word)\n",
    "            tags.append(entity)\n",
    "        return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_tags = read_ner_data(TRAIN_PATH)\n",
    "dev_words, dev_tags = read_ner_data(DEV_PATH)\n",
    "test_words, test_tags = read_ner_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('EU', 'B-ORG'),\n",
      " ('rejects', 'O'),\n",
      " ('German', 'B-MISC'),\n",
      " ('call', 'O'),\n",
      " ('to', 'O'),\n",
      " ('boycott', 'O'),\n",
      " ('British', 'B-MISC'),\n",
      " ('lamb', 'O'),\n",
      " ('.', 'O'),\n",
      " ('Peter', 'B-PER')]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(zip(train_words[:10], train_tags[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that\n",
    "- Each sentence ends with token '.' and tag 'O'. Between sentences there is a blank line.\n",
    "- Same padding and packing pipeline as in the previous lab need be used for the NER data, too.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Build vocabularies for both words and labels (tags)\n",
    "\n",
    "Use *ALL* the data from train, dev, and test sets to build the vocabularies, for word and label (tag), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vocabulary size: 30289\n",
      "Tag vocabulary size: 9\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "def build_vocab(*data_lists):\n",
    "    all_words = [] \n",
    "    all_tags = []  \n",
    "\n",
    "    for data_list in data_lists:\n",
    "        if isinstance(data_list, tuple):\n",
    "            all_words.extend(data_list[0])\n",
    "            all_tags.extend(data_list[1])\n",
    "        else:\n",
    "            all_words.extend(data_list)\n",
    "    \n",
    "    unique_words = set(all_words)\n",
    "    unique_tags = set(all_tags)\n",
    "    \n",
    "    return unique_words, unique_tags\n",
    "\n",
    "vocab_words, vocab_tags = build_vocab((train_words, train_tags), (dev_words, dev_tags), (test_words, test_tags))\n",
    "\n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print('Word vocabulary size:', len(vocab_words))\n",
    "print('Tag vocabulary size:', len(vocab_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "In `__init__` method, initialize `word_embeddings` with a pretrained embedding weight matrix loaded from `glove.6B.100d.txt`.\n",
    "\n",
    "For some variants of model, e.g., maximum entropy Markov model (MEMM), you also need to initialize `tag_embeddings` with a random weight matrix.\n",
    "\n",
    "`forward` method takes the sequence of word indices (and sequece lengths) as input and returns the log probabilities of predicted labels (tags). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, pretrained_embeddings):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "\n",
    "    def forward(self, seq, seq_lens):\n",
    "        embeds = self.word_embeddings(seq)\n",
    "        packed_embeds = nn.utils.rnn.pack_padded_sequence(embeds, seq_lens, batch_first=True, enforce_sorted=False)\n",
    "        lstm_out, _ = self.lstm(packed_embeds)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        tag_space = self.fc(lstm_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=2)\n",
    "        \n",
    "        return tag_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

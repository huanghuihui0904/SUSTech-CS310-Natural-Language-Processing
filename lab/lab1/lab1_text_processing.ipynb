{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS310 Natural Language Processing\n",
    "# Lab 1: Basic Text Processing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines:  4689\n"
     ]
    }
   ],
   "source": [
    "with open(\"三体3死神永生-刘慈欣.txt\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "print('number of lines: ', len(raw))\n",
    "raw = ''.join(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T0. Cleaning the raw data\n",
    "\n",
    "1. Replace the special token `\\u3000` with empty string \"\".\n",
    "2. Replace consecutive newlines with just a single one.\n",
    "3. Other cleaning work you can think of.\n",
    "\n",
    "*Hint*: Use `re.sub()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三体Ⅲ·死神永生刘慈欣\n",
      "写在“基石”之前 姚海军\n",
      "“基石”是个平实的词，不够“炫”，却能够准确传达我们对构建中的中国科幻繁华巨厦的情感与信心，因此，我们用它来作为这套原创丛书的名字。\n",
      "最近十年，是科幻创作飞速发展的十年。王晋康、刘慈欣、何宏伟、韩松等一大批科幻作家发表了大量深受读者喜爱、极具开拓与探索价值的科幻佳作。科幻文学的龙头期刊更是从一本传统的《科幻世界》，发展壮大成为涵盖各个读者层的系列刊物物。与此同时，科幻文学的市场环境也有了改善，省会级城市的大型书店里终于有了属于科幻的领地。\n",
      "仍然有人经常问及中国科幻与美国科幻的差距，但现在的答案已与十年前不同。在很多作品上(它们不再是那种毫无文学技巧与色彩、想象力拘谨的幼稚故事)，这种比较已经变成了人家的牛排之于我们的土豆牛肉。差距是明显的——更准确地说，应该是“差别”——却已经无法再为它们排个名次。口味问题有了实际意义，这正是我们的科幻走向成熟的标志。\n",
      "与美国科幻的差距，实际上是市场化程度的差距。美国科幻从期刊到图书到影视再到游戏和玩具，已经形成了一条完整的产业链，动力十足；而我们的图书出版却仍然处于这样一种局面：读者的阅读需求不能满足\n"
     ]
    }
   ],
   "source": [
    "raw = raw.replace('\\u3000', '')\n",
    "raw = re.sub(r'\\n+', '\\n', raw)\n",
    "# other: removing spaces before and after newlines\n",
    "raw = re.sub(r'\\s*\\n\\s*', '\\n', raw)\n",
    "print(raw[:500]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Count the number of Chinese tokens\n",
    "\n",
    "*Hint*: Use `re.findall()` and the range of Chinese characters in Unicode, i.e., `[\\u4e00-\\u9fa5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chinese tokens: 329946\n"
     ]
    }
   ],
   "source": [
    "chinese_tokens = re.findall(r'[\\u4e00-\\u9fa5]', raw)\n",
    "num_chinese_tokens = len(chinese_tokens)\n",
    "print('Number of Chinese tokens:', num_chinese_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Build the vocabulary for all Chinese tokens\n",
    "\n",
    "Use a Python `dict` object or instance of  `collections.Counter()` to count the frequency of each Chinese token.\n",
    "\n",
    "*Hint*: Go through the `raw` string and for each unique Chinese token, add it to the `dict` or `Counter` object with a count of 1. If the token is already in the `dict` or `Counter` object, increment its count by 1.\n",
    "\n",
    "Check the vocabulary size and print the top 20 most frequent Chinese tokens and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3027\n",
      "Token: 的, Count: 15990\n",
      "Token: 一, Count: 6749\n",
      "Token: 是, Count: 4837\n",
      "Token: 在, Count: 4748\n",
      "Token: 了, Count: 4149\n",
      "Token: 有, Count: 3656\n",
      "Token: 这, Count: 3532\n",
      "Token: 个, Count: 3458\n",
      "Token: 不, Count: 3117\n",
      "Token: 人, Count: 2988\n",
      "Token: 中, Count: 2649\n",
      "Token: 到, Count: 2632\n",
      "Token: 他, Count: 2354\n",
      "Token: 上, Count: 2194\n",
      "Token: 们, Count: 2164\n",
      "Token: 时, Count: 2076\n",
      "Token: 心, Count: 2007\n",
      "Token: 地, Count: 1953\n",
      "Token: 大, Count: 1938\n",
      "Token: 来, Count: 1855\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "vocab = collections.Counter()\n",
    "\n",
    "chinese_tokens = re.findall(r'[\\u4e00-\\u9fa5]', raw)\n",
    "vocab.update(chinese_tokens)\n",
    "\n",
    "vocabulary_size = len(vocab)\n",
    "print('Vocabulary size:', vocabulary_size)\n",
    "\n",
    "top_20_tokens = vocab.most_common(20)\n",
    "for token, count in top_20_tokens:\n",
    "    print(f'Token: {token}, Count: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Sentence segmentation\n",
    "\n",
    "Estimate the number of sentences in the `raw` string by separating the sentences with the delimiter punctuations, such as  `。`, `？`, `！` etc.\n",
    "\n",
    "*Hint*: Use `re.split()` and the correct regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of sentences: 9611\n",
      "三体Ⅲ·死神永生刘慈欣\n",
      "写在“基石”之前 姚海军\n",
      "“基石”是个平实的词，不够“炫”，却能够准确传达我们对构建中的中国科幻繁华巨厦的情感与信心，因此，我们用它来作为这套原创丛书的名字\n",
      "\n",
      "最近十年，是科幻创作飞速发展的十年\n",
      "王晋康、刘慈欣、何宏伟、韩松等一大批科幻作家发表了大量深受读者喜爱、极具开拓与探索价值的科幻佳作\n",
      "科幻文学的龙头期刊更是从一本传统的《科幻世界》，发展壮大成为涵盖各个读者层的系列刊物物\n",
      "与此同时，科幻文学的市场环境也有了改善，省会级城市的大型书店里终于有了属于科幻的领地\n",
      "\n",
      "仍然有人经常问及中国科幻与美国科幻的差距，但现在的答案已与十年前不同\n",
      "在很多作品上(它们不再是那种毫无文学技巧与色彩、想象力拘谨的幼稚故事)，这种比较已经变成了人家的牛排之于我们的土豆牛肉\n",
      "差距是明显的——更准确地说，应该是“差别”——却已经无法再为它们排个名次\n",
      "口味问题有了实际意义，这正是我们的科幻走向成熟的标志\n",
      "\n",
      "与美国科幻的差距，实际上是市场化程度的差距\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "#为啥会不一样\n",
    "\n",
    "sentences = re.split(r'[。？！]+', raw)\n",
    "# remove empty\n",
    "sentences = [sentence for sentence in sentences if sentence.strip()]\n",
    "\n",
    "num_sentences = len(sentences)\n",
    "\n",
    "print('Estimated number of sentences:', num_sentences)\n",
    "for sentence in sentences[:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences obtained with `re.split()` do not contain the delimiter punctuations. What if we want to keep the delimiter punctuations in the sentences?\n",
    "\n",
    "*Hint*: Use `re.findall()` and the correct regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of sentences (with delimiters): 9611\n",
      "First few sentences (with delimiters):\n",
      "三体Ⅲ·死神永生刘慈欣\n",
      "写在“基石”之前 姚海军\n",
      "“基石”是个平实的词，不够“炫”，却能够准确传达我们对构建中的中国科幻繁华巨厦的情感与信心，因此，我们用它来作为这套原创丛书的名字。\n",
      "\n",
      "最近十年，是科幻创作飞速发展的十年。\n",
      "王晋康、刘慈欣、何宏伟、韩松等一大批科幻作家发表了大量深受读者喜爱、极具开拓与探索价值的科幻佳作。\n",
      "科幻文学的龙头期刊更是从一本传统的《科幻世界》，发展壮大成为涵盖各个读者层的系列刊物物。\n",
      "与此同时，科幻文学的市场环境也有了改善，省会级城市的大型书店里终于有了属于科幻的领地。\n",
      "\n",
      "仍然有人经常问及中国科幻与美国科幻的差距，但现在的答案已与十年前不同。\n",
      "在很多作品上(它们不再是那种毫无文学技巧与色彩、想象力拘谨的幼稚故事)，这种比较已经变成了人家的牛排之于我们的土豆牛肉。\n",
      "差距是明显的——更准确地说，应该是“差别”——却已经无法再为它们排个名次。\n",
      "口味问题有了实际意义，这正是我们的科幻走向成熟的标志。\n",
      "\n",
      "与美国科幻的差距，实际上是市场化程度的差距。\n"
     ]
    }
   ],
   "source": [
    "sentences_with_delimiters = re.findall(r'[^。？！]+[。？！]', raw)\n",
    "\n",
    "sentences_with_delimiters = [sentence_with_delimiters for sentence_with_delimiters in sentences_with_delimiters if sentence_with_delimiters.strip()]\n",
    "\n",
    "num_sentences = len(sentences_with_delimiters)\n",
    "print('Estimated number of sentences (with delimiters):', num_sentences)\n",
    "print('First few sentences (with delimiters):')\n",
    "for sentence in sentences_with_delimiters[:10]:\n",
    "    print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4. Count consecutive English and number tokens\n",
    "\n",
    "Estimate the number of consecutive English and number tokens in the `raw` string. Build a vocabulary for them and count their frequency.\n",
    "\n",
    "*Hint*: Use `re.findall()` and the correct regular expression. Use similar method as in T2 to build the vocabulary and count the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique English and number tokens: 172\n",
      "Vocabulary size for English and number tokens: 172\n",
      "Token: AA, Count: 338\n",
      "Token: A, Count: 68\n",
      "Token: I, Count: 66\n",
      "Token: PIA, Count: 45\n",
      "Token: PDC, Count: 35\n",
      "Token: Ice, Count: 34\n",
      "Token: 1, Count: 30\n",
      "Token: IDC, Count: 28\n",
      "Token: DX3906, Count: 27\n",
      "Token: 5, Count: 26\n",
      "Token: 0, Count: 22\n",
      "Token: Way, Count: 20\n",
      "Token: 647, Count: 19\n",
      "Token: 7, Count: 19\n",
      "Token: 3, Count: 15\n",
      "Token: 16, Count: 14\n",
      "Token: 11, Count: 13\n",
      "Token: 4, Count: 12\n",
      "Token: 2, Count: 9\n",
      "Token: 21, Count: 8\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "english_number_tokens = re.findall(r'[a-zA-Z0-9]+', raw)\n",
    "\n",
    "vocab_english_numbers = Counter(english_number_tokens)\n",
    "\n",
    "num_unique_tokens = len(vocab_english_numbers)\n",
    "print('Number of unique English and number tokens:', num_unique_tokens)\n",
    "\n",
    "print('Vocabulary size for English and number tokens:', len(vocab_english_numbers))\n",
    "\n",
    "top_20_tokens = vocab_english_numbers.most_common(20)\n",
    "for token, count in top_20_tokens:\n",
    "    print(f'Token: {token}, Count: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5. Mix of patterns\n",
    "\n",
    "There are two characters whose names are \"艾AA\" and \"程心\". Find all sentences where \"艾AA\" and \"程心\" appear together. Consider fullnames only, that is, \"艾AA\" but not \"AA\" alone. \n",
    "\n",
    "*Hint*: You may find the lookbehind or lookahead pattern useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences with both \"艾AA\" and \"程心\": 16\n",
      "\n",
      "在程心眼中，艾AA是个像鸟一般轻灵的女孩子，充满生机地围着她飞来飞去。\n",
      "\n",
      "程心听到有人叫自己的名字，转身一看，竟是艾AA正向这里跑过来。\n",
      "程心让艾AA在原地等着自己，但AA坚持要随程心去，只好让她上了车。\n",
      "\n",
      "程心和艾AA是随最早的一批移民来到澳大利亚的。\n",
      "\n",
      "#第三部\n",
      "【广播纪元7年，程心】\n",
      "艾AA说程心的眼睛比以前更明亮更美丽了，也许她没有说谎。\n",
      "”坐在程心旁边的艾AA大叫起来，引来众人不满的侧目。\n",
      "\n",
      "这天，艾AA来找程心。\n",
      "\n",
      "是艾AA建议程心报名参加试验的，她认为这是为星环公司参与掩体工程而树立公众形象的一次极佳的免费广告，同时，她和程心都清楚试验是经过严密策划的，只是看上去刺激，基本没什么危险。\n",
      "\n",
      "在返回的途中，当太空艇与地球的距离缩小到三十万千米以内、通信基本没有延时时，程心给艾AA打电话，告诉了她与维德会面的事。\n",
      "\n",
      "与此同时，程心和艾AA进入冬眠。\n",
      "\n",
      "程心到亚洲一号的冬眠中心唤醒了冬眠中的艾AA，两人回到了地球。\n",
      "\n",
      "程心现在身处的世界是一个白色的球形空间，她看到艾AA飘浮在附近，和她一样身穿冬眠时的紧身服，头发湿漉漉的，四肢无力地摊开，显然也是刚刚醒来。\n",
      "对此程心感到很欣慰，到了新世界后，艾AA应该有一个美好的新生活了。\n",
      "\n",
      "程心想到了云天明和艾AA，他们在地面上，应该是安全的，但现在双方已经无法联系，她甚至都没能和他说上一句话。\n",
      "\n",
      "程心和关一帆再次拥抱在一起，他们都为艾AA和云天明流下了欣慰的泪水，幸福地感受着那两个人在十八万个世纪前的幸福，在这种幸福中，他们绝望的心灵变得无比宁静了。\n",
      "”\n",
      "智子的话让程心想到了云天明和艾AA刻在岩石上的字，但关一帆想到的更多，他注意到了智子提到的一个词：田园时代。\n"
     ]
    }
   ],
   "source": [
    "sentences = re.findall(r'[^。？！]+[。？！]', raw)\n",
    "\n",
    "sentences_with_both_names = [sentence for sentence in sentences if \"艾AA\" in sentence and \"程心\" in sentence]\n",
    "\n",
    "print(f'Number of sentences with both \"艾AA\" and \"程心\": {len(sentences_with_both_names)}')\n",
    "\n",
    "for sentence in sentences_with_both_names:\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

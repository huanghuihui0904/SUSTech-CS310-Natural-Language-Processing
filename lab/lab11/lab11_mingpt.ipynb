{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 11: Pretraining MinGPT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task today is to work on top of the Andrej Karpathyâ€™s `minGPT` project (original repo URL: https://github.com/karpathy/minGPT), and define the dataset and training code for pretraining a **character-level** language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from mingpt.model import GPT, GPTConfig\n",
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "from mingpt.utils import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Define the `CharDataset` class\n",
    "\n",
    "`CharDataset` is a subclass of `torch.utils.data.Dataset` that reads a long string of text and returns an iterable sequence of character ID chunks.  \n",
    "\n",
    "The length of the sequence is determined in `__len__` method. The `__getitem__` method takes an integer `idx` as input and returns the `idx`-th **chunk** of character IDs. \n",
    "\n",
    "The size of this chunk is determined by the `block_size` parameter, i.e., the maximum context length for language modeling. The returned `x` and `y` are the character IDs in one chunk, but `y` is shifted by one character.\n",
    "\n",
    "For example, if the input text is \"hello, world!\", and `block_size=4`, then the first chunk will be `x=\"hell\"` and `y=\"ello\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ### START YOUR CODE ###\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        ids = [self.stoi[ch] for ch in chunk]\n",
    "\n",
    "        # Convert to tensor\n",
    "        x = torch.tensor(ids[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(ids[1:], dtype=torch.long)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 12 characters, 9 unique.\n",
      "chunk 0: (tensor([4, 3, 5, 5]), tensor([3, 5, 5, 6]))\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "sample_data = 'hello world!'\n",
    "sample_dataset = CharDataset(sample_data, block_size=4)\n",
    "print('chunk 0:', sample_dataset[0])\n",
    "\n",
    "# You should see the expected output as follows:\n",
    "# data has 12 characters, 9 unique.\n",
    "# chunk 0: (tensor([4, 3, 5, 5]), tensor([3, 5, 5, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Use the provided `trainer`\n",
    "\n",
    "Firstly, load some more serious data, such as some sampled text from Shakespeare's works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "text = open('input.txt', 'r').read()\n",
    "train_dataset = CharDataset(text, block_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, initialize the `GPT` model, with proper hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = GPTConfig(\n",
    "    train_dataset.vocab_size, \n",
    "    train_dataset.block_size,\n",
    "    n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's initialie a `Trainer` and start training! It may take a long time to finish one epoch, but you can stop it at any time.\n",
    "\n",
    "Notes:\n",
    "- The `Trainer` class supports training in multiple processes, but in order to make it work in Jupyter notebook, we set `num_workers=0` and run in a single process.\n",
    "- `ckpt_path` specifies the path to save the model. By default, it saves the model every epoch. Set it to `None` if you don't want to save the model.\n",
    "- No test data is specified, so the thrid argument of `Trainer` is set to `None`.\n",
    "- Explore other parameters as you like in `trainer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfig(max_epochs=2, batch_size=64, \n",
    "                      learning_rate=6e-4, lr_decay=True, \n",
    "                      warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      ckpt_path='mingpt_ckpt.pth', num_workers=0)\n",
    "trainer = Trainer(model, train_dataset, None, trainer_config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also manually save the model by calling `trainer.save_checkpoint()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model saved to `mingpt_ckpt.pth`, though it is not fully trained yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3. Sample from the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`minGPT` provides a `sample` method to generate completions based on a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God!\n",
      "Thoriu ti thad y ar we tot he bed d d withe tes hit tountoundd theild whas,\n",
      "\n",
      "Bithinds t ther as,\n",
      "To\n"
     ]
    }
   ],
   "source": [
    "prompt = \"O God, O God!\"\n",
    "\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in prompt], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 100, temperature=1.0, sample=True, top_k=10)[0]\n",
    "\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course it does not read like Shakespeare at all because your model is not trained enough. \n",
    "\n",
    "What you can do is to load the model trained by somebody else. Download `mingpt_model.pth` from the course website, and load the model weight by `torch.load`. \n",
    "\n",
    "Note that the provided model was trained on GPU, so you need to specify `map_location=torch.device('cpu')` loading it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "pretrained_weight = torch.load('mingpt_model.pth', map_location=torch.device('cpu'))\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print(type(pretrained_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above loaded `pretrained_weight` is merely a dictionary of parameters and not a `GPT` model instance yet. \n",
    "\n",
    "So next, you need to instantiate a new `GPT` model, and load the weights using the `model.load_state_dict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "model_pretrained = GPT(model_config)\n",
    "model_pretrained.load_state_dict(pretrained_weight)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, re-run the generation code to see if there is any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! my lames more?\n",
      "\n",
      "BONELO:\n",
      "There we said, we droth is fast and is.\n",
      "All, in my stracke, which is a the \n"
     ]
    }
   ],
   "source": [
    "prompt = \"O God, O God!\"\n",
    "\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in prompt], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model_pretrained, x, 100, temperature=1.0, sample=True, top_k=10)[0]\n",
    "\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text should read more \"Shakespearean\" than before.\n",
    "\n",
    "Congratulations! You have successfully completed the lab. There are several things you can checkout further:\n",
    "- `minGPT` is no longer actively maintained, but its successor `nanoGPT` is there! Check it out at: https://github.com/karpathy/nanoGPT\n",
    "- As the author claims, `nanoGPT` \"prioritizes teeth over education\", which means you can train your own version of GPT-2 level models, given data and GPU cards. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
